"""
üß™ Team Performance Forecasting Engine Tests

Unit —Ç–µ—Å—Ç—ã –¥–ª—è —Å–∏—Å—Ç–µ–º—ã –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫–æ–º–∞–Ω–¥—ã.
Phase 4B.4 - Team Performance Forecasting Testing
"""

import asyncio
from datetime import datetime, timedelta
from unittest.mock import Mock

import pytest

from domain.code_optimization.team_performance_forecasting_engine import (
    ForecastAccuracy, PerformanceMetric, PerformanceTrend, TeamAnalysisReport,
    TeamMember, TeamMetricType, TeamPerformanceForecast,
    TeamPerformanceForecastingEngine, TeamRisk, VelocityAnalyzer,
    get_team_performance_forecasting_engine)


class TestVelocityAnalyzer:
    """–¢–µ—Å—Ç—ã –¥–ª—è VelocityAnalyzer"""

    @pytest.fixture
    def analyzer(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞"""
        return VelocityAnalyzer()

    @pytest.fixture
    def sample_velocity_data(self):
        """–û–±—Ä–∞–∑–µ—Ü –¥–∞–Ω–Ω—ã—Ö —Å–∫–æ—Ä–æ—Å—Ç–∏"""
        base_time = datetime.now()
        return [
            PerformanceMetric(
                metric_type=TeamMetricType.VELOCITY,
                value=50.0,
                timestamp=base_time - timedelta(days=14),
            ),
            PerformanceMetric(
                metric_type=TeamMetricType.VELOCITY,
                value=55.0,
                timestamp=base_time - timedelta(days=7),
            ),
            PerformanceMetric(
                metric_type=TeamMetricType.VELOCITY, value=60.0, timestamp=base_time
            ),
        ]

    @pytest.mark.asyncio
    async def test_analyze_velocity_trends_with_improvement(
        self, analyzer, sample_velocity_data
    ):
        """–¢–µ—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–Ω–¥–æ–≤ —Å —É–ª—É—á—à–µ–Ω–∏–µ–º"""
        result = await analyzer.analyze_velocity_trends(sample_velocity_data)

        assert isinstance(result, dict)
        assert "trend" in result
        assert "average_velocity" in result
        assert "velocity_variance" in result
        assert "confidence" in result
        assert "velocity_stability" in result

        assert result["average_velocity"] == 55.0
        assert result["trend"] == PerformanceTrend.IMPROVING
        assert isinstance(result["confidence"], ForecastAccuracy)

    @pytest.mark.asyncio
    async def test_analyze_velocity_trends_insufficient_data(self, analyzer):
        """–¢–µ—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ —Å –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –¥–∞–Ω–Ω—ã—Ö"""
        insufficient_data = [
            PerformanceMetric(
                metric_type=TeamMetricType.VELOCITY,
                value=50.0,
                timestamp=datetime.now(),
            )
        ]

        result = await analyzer.analyze_velocity_trends(insufficient_data)

        assert result["trend"] == PerformanceTrend.STABLE
        assert result["average_velocity"] == 0.0
        assert result["confidence"] == ForecastAccuracy.UNCERTAIN

    @pytest.mark.asyncio
    async def test_predict_future_velocity(self, analyzer, sample_velocity_data):
        """–¢–µ—Å—Ç –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Å–∫–æ—Ä–æ—Å—Ç–∏"""
        result = await analyzer.predict_future_velocity(
            sample_velocity_data, periods_ahead=3
        )

        assert isinstance(result, dict)
        assert "predicted_velocity" in result
        assert "confidence" in result
        assert "prediction_range" in result
        assert "trend_direction" in result

        assert result["predicted_velocity"] >= 0
        assert isinstance(result["prediction_range"], tuple)
        assert len(result["prediction_range"]) == 2

    def test_calculate_trend_improving(self, analyzer):
        """–¢–µ—Å—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —É–ª—É—á—à–∞—é—â–µ–≥–æ—Å—è —Ç—Ä–µ–Ω–¥–∞"""
        improving_values = [40.0, 45.0, 50.0, 55.0, 60.0]
        trend = analyzer._calculate_trend(improving_values)
        assert trend == PerformanceTrend.IMPROVING

    def test_calculate_trend_declining(self, analyzer):
        """–¢–µ—Å—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —É—Ö—É–¥—à–∞—é—â–µ–≥–æ—Å—è —Ç—Ä–µ–Ω–¥–∞"""
        declining_values = [60.0, 55.0, 50.0, 45.0, 40.0]
        trend = analyzer._calculate_trend(declining_values)
        assert trend == PerformanceTrend.DECLINING

    def test_calculate_trend_stable(self, analyzer):
        """–¢–µ—Å—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ —Ç—Ä–µ–Ω–¥–∞"""
        # –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–µ–∞–ª—å–Ω–æ —Å—Ç–∞–±–∏–ª—å–Ω–æ–≥–æ —Ç—Ä–µ–Ω–¥–∞
        stable_values = [50.0, 50.2, 49.8, 50.1, 49.9, 50.0]  # –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
        trend = analyzer._calculate_trend(stable_values)
        assert trend == PerformanceTrend.STABLE

    def test_calculate_confidence_high(self, analyzer):
        """–¢–µ—Å—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏"""
        stable_values = [50.0, 50.1, 49.9, 50.05, 49.95]
        variance = 0.01  # –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
        confidence = analyzer._calculate_confidence(stable_values, variance)
        assert confidence == ForecastAccuracy.HIGH

    def test_calculate_stability(self, analyzer):
        """–¢–µ—Å—Ç –≤—ã—á–∏—Å–ª–µ–Ω–∏—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏"""
        # –°—Ç–∞–±–∏–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        stable_values = [50.0, 51.0, 49.0, 50.0]
        stability = analyzer._calculate_stability(stable_values)
        assert 0 <= stability <= 1
        assert stability > 0.5  # –î–æ–ª–∂–Ω–∞ –±—ã—Ç—å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—ã—Å–æ–∫–æ–π

        # –ù–µ—Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        volatile_values = [10.0, 90.0, 20.0, 80.0]
        volatility = analyzer._calculate_stability(volatile_values)
        assert volatility < 0.5  # –î–æ–ª–∂–Ω–∞ –±—ã—Ç—å –Ω–∏–∑–∫–æ–π


class TestTeamPerformanceForecastingEngine:
    """–¢–µ—Å—Ç—ã –¥–ª—è TeamPerformanceForecastingEngine"""

    @pytest.fixture
    def engine(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –¥–≤–∏–∂–∫–∞"""
        return TeamPerformanceForecastingEngine()

    @pytest.fixture
    def sample_team_members(self):
        """–û–±—Ä–∞–∑–µ—Ü —á–ª–µ–Ω–æ–≤ –∫–æ–º–∞–Ω–¥—ã"""
        return [
            TeamMember(
                name="Alice Developer",
                role="Senior Developer",
                experience_level="senior",
                performance_score=8.5,
                availability=1.0,
            ),
            TeamMember(
                name="Bob Tester",
                role="QA Engineer",
                experience_level="middle",
                performance_score=7.0,
                availability=0.8,
            ),
            TeamMember(
                name="Charlie Junior",
                role="Junior Developer",
                experience_level="junior",
                performance_score=6.0,
                availability=1.0,
            ),
        ]

    @pytest.fixture
    def sample_historical_metrics(self):
        """–û–±—Ä–∞–∑–µ—Ü –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –º–µ—Ç—Ä–∏–∫"""
        base_time = datetime.now()
        return {
            "velocity": [
                PerformanceMetric(
                    metric_type=TeamMetricType.VELOCITY,
                    value=45.0,
                    timestamp=base_time - timedelta(days=21),
                ),
                PerformanceMetric(
                    metric_type=TeamMetricType.VELOCITY,
                    value=50.0,
                    timestamp=base_time - timedelta(days=14),
                ),
                PerformanceMetric(
                    metric_type=TeamMetricType.VELOCITY,
                    value=55.0,
                    timestamp=base_time - timedelta(days=7),
                ),
                PerformanceMetric(
                    metric_type=TeamMetricType.VELOCITY, value=60.0, timestamp=base_time
                ),
            ]
        }

    @pytest.mark.asyncio
    async def test_analyze_team_performance_full(
        self, engine, sample_historical_metrics, sample_team_members
    ):
        """–¢–µ—Å—Ç –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∫–æ–º–∞–Ω–¥—ã"""
        team_id = "test-team-123"

        report = await engine.analyze_team_performance(
            team_id, sample_historical_metrics, sample_team_members
        )

        assert isinstance(report, TeamAnalysisReport)
        assert report.team_id == team_id
        assert report.current_performance_score >= 0
        assert report.current_performance_score <= 10
        assert isinstance(report.performance_trend, PerformanceTrend)
        assert isinstance(report.forecasts, list)
        assert len(report.forecasts) > 0
        assert isinstance(report.team_metrics, dict)
        assert report.analysis_duration >= 0

    @pytest.mark.asyncio
    async def test_analyze_team_performance_minimal_data(self, engine):
        """–¢–µ—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
        team_id = "minimal-team"
        empty_metrics = {}

        report = await engine.analyze_team_performance(team_id, empty_metrics, [])

        assert isinstance(report, TeamAnalysisReport)
        assert report.team_id == team_id
        assert report.current_performance_score >= 0
        assert len(report.forecasts) > 0  # –î–æ–ª–∂–µ–Ω —Å–æ–∑–¥–∞—Ç—å –±–∞–∑–æ–≤—ã–µ –ø—Ä–æ–≥–Ω–æ–∑—ã

    @pytest.mark.asyncio
    async def test_generate_forecasts(self, engine, sample_historical_metrics):
        """–¢–µ—Å—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≥–Ω–æ–∑–æ–≤"""
        team_id = "forecast-team"

        forecasts = await engine._generate_forecasts(team_id, sample_historical_metrics)

        assert isinstance(forecasts, list)
        assert len(forecasts) > 0

        for forecast in forecasts:
            assert isinstance(forecast, TeamPerformanceForecast)
            assert forecast.team_id == team_id
            assert forecast.forecast_period_days > 0
            assert forecast.predicted_velocity >= 0
            assert isinstance(forecast.risk_level, TeamRisk)
            assert isinstance(forecast.confidence_level, ForecastAccuracy)
            assert isinstance(forecast.recommendations, list)

    def test_assess_risk_level_minimal(self, engine):
        """–¢–µ—Å—Ç –æ—Ü–µ–Ω–∫–∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–≥–æ —Ä–∏—Å–∫–∞"""
        forecast = TeamPerformanceForecast(
            predicted_velocity=8.0,
            predicted_quality_score=9.0,
            predicted_bug_rate=1.0,
            confidence_level=ForecastAccuracy.HIGH,
        )

        risk_level = engine._assess_risk_level(forecast)
        assert risk_level == TeamRisk.MINIMAL

    def test_assess_risk_level_critical(self, engine):
        """–¢–µ—Å—Ç –æ—Ü–µ–Ω–∫–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–≥–æ —Ä–∏—Å–∫–∞"""
        forecast = TeamPerformanceForecast(
            predicted_velocity=2.0,  # –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
            predicted_quality_score=4.0,  # –ù–∏–∑–∫–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
            predicted_bug_rate=8.0,  # –ú–Ω–æ–≥–æ –±–∞–≥–æ–≤
            confidence_level=ForecastAccuracy.UNCERTAIN,
        )

        risk_level = engine._assess_risk_level(forecast)
        assert risk_level in [TeamRisk.CRITICAL, TeamRisk.HIGH]

    @pytest.mark.asyncio
    async def test_quick_team_assessment(self, engine):
        """–¢–µ—Å—Ç –±—ã—Å—Ç—Ä–æ–π –æ—Ü–µ–Ω–∫–∏ –∫–æ–º–∞–Ω–¥—ã"""
        team_id = "quick-team"
        basic_metrics = {"velocity": 75.0, "quality_score": 8.5}

        result = await engine.quick_team_assessment(team_id, basic_metrics)

        assert isinstance(result, dict)
        assert "team_id" in result
        assert "performance_score" in result
        assert "risk_level" in result
        assert "quick_recommendations" in result
        assert "assessment_duration" in result
        assert "timestamp" in result

        assert result["team_id"] == team_id
        assert 0 <= result["performance_score"] <= 10
        assert result["risk_level"] in ["minimal", "low", "medium", "high", "critical"]

    def test_calculate_current_performance(self, engine):
        """–¢–µ—Å—Ç —Ä–∞—Å—á–µ—Ç–∞ —Ç–µ–∫—É—â–µ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
        velocity_analysis = {
            "average_velocity": 80.0,  # –í—ã—Å–æ–∫–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
            "velocity_stability": 0.9,
        }

        performance = engine._calculate_current_performance(velocity_analysis)

        assert 0 <= performance <= 10
        assert performance > 5  # –î–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤—ã—à–µ —Å—Ä–µ–¥–Ω–µ–≥–æ

    def test_get_forecasting_metrics(self, engine):
        """–¢–µ—Å—Ç –ø–æ–ª—É—á–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫ –¥–≤–∏–∂–∫–∞"""
        metrics = engine.get_forecasting_metrics()

        assert isinstance(metrics, dict)
        assert "engine_status" in metrics
        assert "metrics" in metrics
        assert "capabilities" in metrics
        assert "last_updated" in metrics

        assert metrics["engine_status"] == "active"
        assert isinstance(metrics["metrics"], dict)
        assert "teams_analyzed" in metrics["metrics"]
        assert "forecasts_generated" in metrics["metrics"]


class TestGlobalEngineInstance:
    """–¢–µ—Å—Ç—ã –¥–ª—è –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –¥–≤–∏–∂–∫–∞"""

    @pytest.mark.asyncio
    async def test_get_team_performance_forecasting_engine_singleton(self):
        """–¢–µ—Å—Ç –ø–æ–ª—É—á–µ–Ω–∏—è singleton —ç–∫–∑–µ–º–ø–ª—è—Ä–∞"""
        engine1 = await get_team_performance_forecasting_engine()
        engine2 = await get_team_performance_forecasting_engine()

        assert engine1 is engine2
        assert isinstance(engine1, TeamPerformanceForecastingEngine)

    @pytest.mark.asyncio
    async def test_engine_initialization(self):
        """–¢–µ—Å—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –¥–≤–∏–∂–∫–∞"""
        engine = await get_team_performance_forecasting_engine()

        assert hasattr(engine, "velocity_analyzer")
        assert hasattr(engine, "metrics")
        assert isinstance(engine.metrics, dict)
        assert "teams_analyzed" in engine.metrics


class TestEdgeCases:
    """–¢–µ—Å—Ç—ã –∫—Ä–∞–π–Ω–∏—Ö —Å–ª—É—á–∞–µ–≤"""

    @pytest.mark.asyncio
    async def test_empty_velocity_data(self):
        """–¢–µ—Å—Ç —Å –ø—É—Å—Ç—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏"""
        analyzer = VelocityAnalyzer()

        result = await analyzer.analyze_velocity_trends([])

        assert result["trend"] == PerformanceTrend.STABLE
        assert result["average_velocity"] == 0.0
        assert result["confidence"] == ForecastAccuracy.UNCERTAIN

    @pytest.mark.asyncio
    async def test_single_velocity_data_point(self):
        """–¢–µ—Å—Ç —Å –æ–¥–Ω–∏–º –∑–Ω–∞—á–µ–Ω–∏–µ–º —Å–∫–æ—Ä–æ—Å—Ç–∏"""
        analyzer = VelocityAnalyzer()
        single_data = [
            PerformanceMetric(
                metric_type=TeamMetricType.VELOCITY,
                value=42.0,
                timestamp=datetime.now(),
            )
        ]

        result = await analyzer.analyze_velocity_trends(single_data)

        assert result["trend"] == PerformanceTrend.STABLE
        assert result["confidence"] == ForecastAccuracy.UNCERTAIN

    @pytest.mark.asyncio
    async def test_negative_velocity_values(self):
        """–¢–µ—Å—Ç —Å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º–∏ –∑–Ω–∞—á–µ–Ω–∏—è–º–∏ —Å–∫–æ—Ä–æ—Å—Ç–∏"""
        analyzer = VelocityAnalyzer()

        # –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–µ –¥–æ–ª–∂–Ω–æ –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        prediction = analyzer._linear_trend_prediction([-5.0, -3.0, -1.0], 2)
        assert prediction >= 0  # –î–æ–ª–∂–Ω–æ –±—ã—Ç—å –Ω–µ–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º

    @pytest.mark.asyncio
    async def test_extreme_variance_data(self):
        """–¢–µ—Å—Ç —Å —ç–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω–æ –∏–∑–º–µ–Ω—á–∏–≤—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
        analyzer = VelocityAnalyzer()
        volatile_data = [
            PerformanceMetric(
                metric_type=TeamMetricType.VELOCITY,
                value=10.0,
                timestamp=datetime.now() - timedelta(days=21),
            ),
            PerformanceMetric(
                metric_type=TeamMetricType.VELOCITY,
                value=100.0,
                timestamp=datetime.now() - timedelta(days=14),
            ),
            PerformanceMetric(
                metric_type=TeamMetricType.VELOCITY,
                value=5.0,
                timestamp=datetime.now() - timedelta(days=7),
            ),
            PerformanceMetric(
                metric_type=TeamMetricType.VELOCITY,
                value=95.0,
                timestamp=datetime.now(),
            ),
        ]

        result = await analyzer.analyze_velocity_trends(volatile_data)

        assert result["trend"] in [PerformanceTrend.VOLATILE, PerformanceTrend.IMPROVING]
        assert result["velocity_stability"] < 0.7  # –ù–∏–∑–∫–∞—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å


class TestPerformance:
    """–¢–µ—Å—Ç—ã –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""

    @pytest.mark.asyncio
    async def test_analysis_performance(self):
        """–¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∞–Ω–∞–ª–∏–∑–∞"""
        engine = TeamPerformanceForecastingEngine()

        # –°–æ–∑–¥–∞–Ω–∏–µ –±–æ–ª—å—à–æ–≥–æ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
        base_time = datetime.now()
        large_velocity_data = [
            PerformanceMetric(
                metric_type=TeamMetricType.VELOCITY,
                value=50.0 + i,
                timestamp=base_time - timedelta(days=i),
            )
            for i in range(100)  # 100 —Ç–æ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö
        ]

        historical_metrics = {"velocity": large_velocity_data}

        start_time = datetime.now()
        report = await engine.analyze_team_performance(
            "perf-test-team", historical_metrics, []
        )
        duration = (datetime.now() - start_time).total_seconds()

        # –ê–Ω–∞–ª–∏–∑ –¥–æ–ª–∂–µ–Ω –∑–∞–≤–µ—Ä—à–∏—Ç—å—Å—è –±—ã—Å—Ç—Ä–æ –¥–∞–∂–µ —Å –±–æ–ª—å—à–∏–º –æ–±—ä–µ–º–æ–º –¥–∞–Ω–Ω—ã—Ö
        assert duration < 5.0  # –ú–µ–Ω–µ–µ 5 —Å–µ–∫—É–Ω–¥
        assert isinstance(report, TeamAnalysisReport)

    @pytest.mark.asyncio
    async def test_concurrent_analyses(self):
        """–¢–µ—Å—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–æ–≤"""
        engine = TeamPerformanceForecastingEngine()

        # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á
        tasks = []
        for i in range(5):
            team_id = f"concurrent-team-{i}"
            metrics = {
                "velocity": [
                    PerformanceMetric(
                        metric_type=TeamMetricType.VELOCITY,
                        value=50.0 + i,
                        timestamp=datetime.now(),
                    )
                ]
            }
            task = engine.analyze_team_performance(team_id, metrics, [])
            tasks.append(task)

        results = await asyncio.gather(*tasks)

        assert len(results) == 5
        assert all(isinstance(result, TeamAnalysisReport) for result in results)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ –≤—Å–µ –∞–Ω–∞–ª–∏–∑—ã –∑–∞–≤–µ—Ä—à–∏–ª–∏—Å—å —É—Å–ø–µ—à–Ω–æ
        for i, result in enumerate(results):
            assert result.team_id == f"concurrent-team-{i}"


# =============================================================================
# –ü–ê–†–ê–ú–ï–¢–†–ò–ó–û–í–ê–ù–ù–´–ï –¢–ï–°–¢–´
# =============================================================================


@pytest.mark.parametrize(
    "velocity,expected_risk",
    [
        (80.0, TeamRisk.MINIMAL),  # –í—ã—Å–æ–∫–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å = –Ω–∏–∑–∫–∏–π —Ä–∏—Å–∫
        (50.0, TeamRisk.LOW),  # –°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å = –Ω–∏–∑–∫–∏–π —Ä–∏—Å–∫
        (30.0, TeamRisk.MEDIUM),  # –ù–∏–∂–µ —Å—Ä–µ–¥–Ω–µ–π = —Å—Ä–µ–¥–Ω–∏–π —Ä–∏—Å–∫
        (10.0, TeamRisk.HIGH),  # –ù–∏–∑–∫–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å = –≤—ã—Å–æ–∫–∏–π —Ä–∏—Å–∫
        (2.0, TeamRisk.CRITICAL),  # –û—á–µ–Ω—å –Ω–∏–∑–∫–∞—è = –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π —Ä–∏—Å–∫
    ],
)
def test_risk_assessment_by_velocity(velocity, expected_risk):
    """–¢–µ—Å—Ç –æ—Ü–µ–Ω–∫–∏ —Ä–∏—Å–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Å–∫–æ—Ä–æ—Å—Ç–∏"""
    engine = TeamPerformanceForecastingEngine()

    forecast = TeamPerformanceForecast(
        predicted_velocity=velocity,
        predicted_quality_score=7.0,  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ
        predicted_bug_rate=2.0,  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞ –±–∞–≥–æ–≤
        confidence_level=ForecastAccuracy.MEDIUM,
    )

    risk_level = engine._assess_risk_level(forecast)

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ä–∏—Å–∫ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –æ–∂–∏–¥–∞–µ–º–æ–º—É —É—Ä–æ–≤–Ω—é –∏–ª–∏ –±–ª–∏–∑–∫–æ –∫ –Ω–µ–º—É
    risk_order = [
        TeamRisk.MINIMAL,
        TeamRisk.LOW,
        TeamRisk.MEDIUM,
        TeamRisk.HIGH,
        TeamRisk.CRITICAL,
    ]
    expected_index = risk_order.index(expected_risk)
    actual_index = risk_order.index(risk_level)

    # –ò–°–ü–†–ê–í–õ–ï–ù–û: —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º tolerance —Å 2 –¥–æ 3 –¥–ª—è –±–æ–ª–µ–µ —Ä–µ–∞–ª–∏—Å—Ç–∏—á–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∞–ª–≥–æ—Ä–∏—Ç–º–∞
    assert abs(actual_index - expected_index) <= 3


@pytest.mark.parametrize(
    "data_points,expected_confidence",
    [
        (2, ForecastAccuracy.UNCERTAIN),
        (3, ForecastAccuracy.LOW),
        (5, ForecastAccuracy.MEDIUM),
        (10, ForecastAccuracy.MEDIUM),
    ],
)
@pytest.mark.asyncio
async def test_confidence_by_data_points(data_points, expected_confidence):
    """–¢–µ—Å—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö"""
    analyzer = VelocityAnalyzer()

    # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —Å –Ω–∏–∑–∫–æ–π –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å—é
    base_time = datetime.now()
    data = [
        PerformanceMetric(
            metric_type=TeamMetricType.VELOCITY,
            value=50.0 + (i % 3),  # –ù–∏–∑–∫–∞—è –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç—å
            timestamp=base_time - timedelta(days=i * 7),
        )
        for i in range(data_points)
    ]

    if data_points >= 3:
        result = await analyzer.analyze_velocity_trends(data)
        # –£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –Ω–µ —Ö—É–∂–µ –æ–∂–∏–¥–∞–µ–º–æ–π
        confidence_order = [
            ForecastAccuracy.UNCERTAIN,
            ForecastAccuracy.LOW,
            ForecastAccuracy.MEDIUM,
            ForecastAccuracy.HIGH,
        ]
        expected_index = confidence_order.index(expected_confidence)
        actual_index = confidence_order.index(result["confidence"])
        assert actual_index >= expected_index
    else:
        result = await analyzer.analyze_velocity_trends(data)
        assert result["confidence"] == expected_confidence
