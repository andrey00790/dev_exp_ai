#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ingest_bootstrap.py

–ú–æ–¥—É–ª—å –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ bootstrap –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –≤ Qdrant.

–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:
- –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ñ–∞–π–ª–æ–≤ –∏–∑ –ø–∞–ø–∫–∏ bootstrap
- –ü–æ–¥–¥–µ—Ä–∂–∫–∞ PDF, TXT, ZIP –∞—Ä—Ö–∏–≤–æ–≤
- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏–∑ .json —Ñ–∞–π–ª–æ–≤
- –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Å–∏—Å—Ç–µ–º–æ–π –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –¥–∞–Ω–Ω—ã—Ö
"""

import os
import sys
import json
import zipfile
import tempfile
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional, Generator, Tuple
import hashlib
import uuid
import textwrap

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ –º–æ–¥—É–ª—è–º –ø—Ä–æ–µ–∫—Ç–∞
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

try:
    from models.data_source import (
        SourceType, DataSourceManager, SourceMetadata, 
        create_source_metadata_from_file, get_data_source_manager
    )
except ImportError:
    # Fallback –µ—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞
    print("Warning: data_source model not available, using fallback")
    SourceType = type('SourceType', (), {'BOOTSTRAP': 'bootstrap'})()

logger = logging.getLogger("ingest_bootstrap")

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
NAMESPACE = uuid.UUID("12345678-1234-5678-1234-567812345678")

def sha_str(s: str) -> str:
    """–°–æ–∑–¥–∞–µ—Ç SHA-1 —Ö–µ—à —Å—Ç—Ä–æ–∫–∏"""
    return hashlib.sha1(s.encode()).hexdigest()

def sha_uuid(s: str) -> str:
    """–°–æ–∑–¥–∞–µ—Ç UUID5 –∏–∑ —Å—Ç—Ä–æ–∫–∏"""
    return str(uuid.uuid5(NAMESPACE, s))

def wrap_text(text: str, chunk_size: int = 1500) -> List[str]:
    """–†–∞–∑–±–∏–≤–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏"""
    return textwrap.wrap(text, chunk_size, break_long_words=False, break_on_hyphens=False)


class BootstrapProcessor:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ bootstrap –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤"""
    
    def __init__(self, bootstrap_dir: str = "bootstrap"):
        self.bootstrap_dir = Path(bootstrap_dir)
        self.data_source_manager = get_data_source_manager() if 'get_data_source_manager' in globals() else None
        self.stats = {
            "files_processed": 0,
            "chunks_created": 0,
            "errors": 0,
            "categories": {}
        }
        
    def extract_text_from_pdf(self, pdf_path: Path) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ PDF —Ñ–∞–π–ª–∞"""
        try:
            import PyPDF2
            text = ""
            with open(pdf_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text += page.extract_text() + "\n"
            return text
        except ImportError:
            logger.warning("PyPDF2 –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º PDF —Ñ–∞–π–ª")
            return f"PDF —Ñ–∞–π–ª: {pdf_path.name}\n–¢—Ä–µ–±—É–µ—Ç—Å—è PyPDF2 –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞."
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ PDF {pdf_path}: {e}")
            return f"PDF —Ñ–∞–π–ª: {pdf_path.name}\n–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–∞: {e}"
            
    def extract_text_from_zip(self, zip_path: Path) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∏–∑ ZIP –∞—Ä—Ö–∏–≤–∞ (—Ñ–∞–π–ª—ã README, markdown –∏ —Ç.–¥.)"""
        text_content = []
        
        try:
            with zipfile.ZipFile(zip_path, 'r') as zip_file:
                # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∏–Ω—Ç–µ—Ä–µ—Å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
                interesting_files = []
                for file_info in zip_file.infolist():
                    if not file_info.is_dir():
                        name = file_info.filename.lower()
                        if any(ext in name for ext in [
                            'readme', '.md', '.txt', '.rst', 
                            'doc', 'guide', 'tutorial'
                        ]):
                            interesting_files.append(file_info)
                
                # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
                interesting_files = interesting_files[:20]
                
                for file_info in interesting_files:
                    try:
                        with zip_file.open(file_info) as file:
                            content = file.read()
                            # –ü—ã—Ç–∞–µ–º—Å—è –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –∫–∞–∫ UTF-8
                            try:
                                text = content.decode('utf-8')
                            except UnicodeDecodeError:
                                text = content.decode('latin-1', errors='ignore')
                                
                            text_content.append(f"\n--- {file_info.filename} ---\n")
                            text_content.append(text)
                            
                    except Exception as e:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Ñ–∞–π–ª {file_info.filename} –∏–∑ –∞—Ä—Ö–∏–≤–∞: {e}")
                        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ ZIP –∞—Ä—Ö–∏–≤–∞ {zip_path}: {e}")
            return f"ZIP –∞—Ä—Ö–∏–≤: {zip_path.name}\n–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {e}"
            
        return "\n".join(text_content) if text_content else f"ZIP –∞—Ä—Ö–∏–≤: {zip_path.name}\n–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ"
        
    def extract_text_from_txt(self, txt_path: Path) -> str:
        """–ß–∏—Ç–∞–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞"""
        try:
            with open(txt_path, 'r', encoding='utf-8') as file:
                return file.read()
        except UnicodeDecodeError:
            try:
                with open(txt_path, 'r', encoding='latin-1') as file:
                    return file.read()
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ç–µ–∫—Å—Ç–æ–≤–æ–≥–æ —Ñ–∞–π–ª–∞ {txt_path}: {e}")
                return f"–¢–µ–∫—Å—Ç–æ–≤—ã–π —Ñ–∞–π–ª: {txt_path.name}\n–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏: {e}"
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {txt_path}: {e}")
            return f"–§–∞–π–ª: {txt_path.name}\n–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏: {e}"
            
    def load_metadata(self, file_path: Path) -> Optional[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–≥–æ .json —Ñ–∞–π–ª–∞"""
        metadata_path = file_path.with_suffix('.json')
        if metadata_path.exists():
            try:
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ {metadata_path}: {e}")
        return None
        
    def process_file(self, file_path: Path) -> Generator[Tuple[str, str, Dict[str, Any]], None, None]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —Ñ–∞–π–ª –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∞–Ω–∫–∏ –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏"""
        
        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª—ã –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ —Ñ–∞–π–ª—ã
        if file_path.suffix == '.json' or file_path.name in ['download_stats.json', 'README.md', 'example_guide.txt']:
            return
            
        logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª: {file_path}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        metadata = self.load_metadata(file_path)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ —Ñ–∞–π–ª–∞
        if file_path.suffix.lower() == '.pdf':
            text_content = self.extract_text_from_pdf(file_path)
        elif file_path.suffix.lower() == '.zip':
            text_content = self.extract_text_from_zip(file_path)
        elif file_path.suffix.lower() in ['.txt', '.md', '.rst']:
            text_content = self.extract_text_from_txt(file_path)
        else:
            logger.warning(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ç–∏–ø —Ñ–∞–π–ª–∞: {file_path}")
            return
            
        if not text_content or len(text_content.strip()) < 10:
            logger.warning(f"–°–ª–∏—à–∫–æ–º –º–∞–ª–æ —Ç–µ–∫—Å—Ç–∞ –≤ —Ñ–∞–π–ª–µ {file_path}")
            return
            
        # –°–æ–∑–¥–∞–µ–º –±–∞–∑–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–ª—è payload
        base_payload = {
            "src": "bootstrap",
            "file_name": file_path.name,
            "file_path": str(file_path),
            "file_size": file_path.stat().st_size,
            "category": "general"
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏–∑ .json —Ñ–∞–π–ª–∞ –µ—Å–ª–∏ –µ—Å—Ç—å
        if metadata:
            base_payload.update({
                "name": metadata.get('name', file_path.stem),
                "category": metadata.get('category', 'general'),
                "role": metadata.get('role'),
                "original_url": metadata.get('url'),
                "url_type": metadata.get('url_type'),
                "downloaded_at": metadata.get('downloaded_at')
            })
            
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        category = base_payload.get('category', 'general')
        if category not in self.stats["categories"]:
            self.stats["categories"][category] = 0
        self.stats["categories"][category] += 1
        
        # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞–Ω–∫–∏
        chunks = wrap_text(text_content)
        
        for i, chunk in enumerate(chunks):
            if len(chunk.strip()) < 20:  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–µ —á–∞–Ω–∫–∏
                continue
                
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–π ID –¥–ª—è —á–∞–Ω–∫–∞
            chunk_id = f"{file_path.stem}_{i}"
            sha_hex = sha_str(f"bootstrap:{chunk_id}:{chunk[:50]}")
            uid = sha_uuid(sha_hex)
            
            # –°–æ–∑–¥–∞–µ–º payload –¥–ª—è —á–∞–Ω–∫–∞
            chunk_payload = base_payload.copy()
            chunk_payload.update({
                "chunk_id": chunk_id,
                "chunk_index": i,
                "text": chunk,
                "url": f"bootstrap://{file_path.name}#{chunk_id}"
            })
            
            yield uid, chunk, chunk_payload
            
        self.stats["files_processed"] += 1
        logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω —Ñ–∞–π–ª {file_path}: {len(chunks)} —á–∞–Ω–∫–æ–≤")
        
    def process_directory(self, category_dir: Path) -> Generator[Tuple[str, str, Dict[str, Any]], None, None]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –†–ï–ö–£–†–°–ò–í–ù–û"""
        if not category_dir.exists() or not category_dir.is_dir():
            return
            
        logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é: {category_dir.name}")
        
        # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö
        for file_path in category_dir.rglob("*"):
            if file_path.is_file():
                try:
                    yield from self.process_file(file_path)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {file_path}: {e}")
                    self.stats["errors"] += 1
                    
    def ingest_all_bootstrap(self) -> Generator[Tuple[str, str, Dict[str, Any]], None, None]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ bootstrap –º–∞—Ç–µ—Ä–∏–∞–ª—ã –†–ï–ö–£–†–°–ò–í–ù–û"""
        logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –†–ï–ö–£–†–°–ò–í–ù–£–Æ –æ–±—Ä–∞–±–æ—Ç–∫—É bootstrap –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∏–∑ {self.bootstrap_dir}")
        
        if not self.bootstrap_dir.exists():
            logger.warning(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è bootstrap –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {self.bootstrap_dir}")
            return
            
        # –†–µ–∫—É—Ä—Å–∏–≤–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –í–°–ï —Ñ–∞–π–ª—ã –≤ bootstrap –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –∏ –≤—Å–µ—Ö –ø–æ–¥–¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è—Ö
        for file_path in self.bootstrap_dir.rglob("*"):
            if file_path.is_file():
                try:
                    yield from self.process_file(file_path)
                except Exception as e:
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {file_path}: {e}")
                    self.stats["errors"] += 1
                    
        logger.info("–ó–∞–≤–µ—Ä—à–µ–Ω–∞ –†–ï–ö–£–†–°–ò–í–ù–ê–Ø –æ–±—Ä–∞–±–æ—Ç–∫–∞ bootstrap –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤")
        self.print_stats()
        
    def print_stats(self):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        logger.info("‚ïê‚ïê‚ïê –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò BOOTSTRAP ‚ïê‚ïê‚ïê")
        logger.info(f"–§–∞–π–ª–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.stats['files_processed']}")
        logger.info(f"–ß–∞–Ω–∫–æ–≤ —Å–æ–∑–¥–∞–Ω–æ: {self.stats['chunks_created']}")
        logger.info(f"–û—à–∏–±–æ–∫: {self.stats['errors']}")
        logger.info("–ö–∞—Ç–µ–≥–æ—Ä–∏–∏:")
        for category, count in self.stats["categories"].items():
            logger.info(f"  {category}: {count} —Ñ–∞–π–ª–æ–≤")
        logger.info("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")


def ingest_bootstrap_materials(bootstrap_dir: str = "bootstrap") -> Generator[Tuple[str, str, Dict[str, Any]], None, None]:
    """
    –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å ingest_data.py
    
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–æ—Ä—Ç–µ–∂–µ–π (uid, text, payload) –¥–ª—è –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏
    """
    processor = BootstrapProcessor(bootstrap_dir)
    
    for uid, text, payload in processor.ingest_all_bootstrap():
        processor.stats["chunks_created"] += 1
        yield uid, text, payload


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ ingest_data.py:
def test_bootstrap_ingestion():
    """–¢–µ—Å—Ç–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ bootstrap –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤"""
    logger.info("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ bootstrap –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤...")
    
    count = 0
    for uid, text, payload in ingest_bootstrap_materials():
        count += 1
        if count <= 3:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3 —á–∞–Ω–∫–∞
            logger.info(f"UID: {uid}")
            logger.info(f"–ö–∞—Ç–µ–≥–æ—Ä–∏—è: {payload.get('category')}")
            logger.info(f"–§–∞–π–ª: {payload.get('file_name')}")
            logger.info(f"–¢–µ–∫—Å—Ç (–ø–µ—Ä–≤—ã–µ 100 —Å–∏–º–≤–æ–ª–æ–≤): {text[:100]}...")
            logger.info("---")
            
    logger.info(f"–í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {count}")


if __name__ == "__main__":
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s [%(levelname)s] %(name)s: %(message)s"
    )
    
    test_bootstrap_ingestion() 