"""
Learning Pipeline Service for AI Assistant.

–†–µ–∞–ª–∏–∑—É–µ—Ç –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è —Å –ø–æ–¥–∫—Ä–µ–ø–ª–µ–Ω–∏–µ–º:
1. –°–±–æ—Ä —Ñ–∏–¥–±–µ–∫–∞ –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ RFC
2. –ê–Ω–∞–ª–∏–∑ –∫–∞—á–µ—Å—Ç–≤–∞ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
3. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
4. Continuous improvement –ø—Ä–æ—Ü–µ—Å—Å
"""

import asyncio
import logging
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple

from adapters.llm.llm_loader import load_llm

from domain.monitoring.feedback_service import get_feedback_service
from models.feedback import FeedbackContext, FeedbackType, UserFeedback
from models.generation import GeneratedRFC, GenerationSession

logger = logging.getLogger(__name__)


class LearningQuality(Enum):
    """–ö–∞—á–µ—Å—Ç–≤–æ –æ–±—É—á–∞—é—â–µ–≥–æ –ø—Ä–∏–º–µ—Ä–∞."""

    EXCELLENT = "excellent"
    GOOD = "good"
    POOR = "poor"
    INVALID = "invalid"


@dataclass
class LearningExample:
    """–ü—Ä–∏–º–µ—Ä –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏."""

    session_id: str
    rfc_id: str
    input_context: str  # –ó–∞–ø—Ä–æ—Å + –æ—Ç–≤–µ—Ç—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    generated_content: str  # –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
    feedback_score: float  # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ (0.0-1.0)
    quality: LearningQuality  # –ö–∞—Ç–µ–≥–æ—Ä–∏—è –∫–∞—á–µ—Å—Ç–≤–∞
    user_comments: List[str]  # –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
    improvement_suggestions: str  # –ü—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –ø–æ —É–ª—É—á—à–µ–Ω–∏—é
    created_at: datetime


@dataclass
class RetrainingTrigger:
    """–¢—Ä–∏–≥–≥–µ—Ä –¥–ª—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏."""

    trigger_type: str  # 'feedback_threshold', 'time_based', 'manual'
    threshold_reached: bool
    examples_count: int
    average_quality: float
    last_retraining: Optional[datetime]


class LearningPipelineService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ü–∏–∫–ª–∞ –æ–±—É—á–µ–Ω–∏—è AI Assistant."""

    def __init__(self):
        self.feedback_service = get_feedback_service()
        self.llm = load_llm()

        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
        self.min_examples_for_retraining = 10  # –£–º–µ–Ω—å—à–∏–ª –¥–ª—è –¥–µ–º–æ
        self.quality_threshold = 0.7
        self.retraining_interval_days = 1  # –£–º–µ–Ω—å—à–∏–ª –¥–ª—è –¥–µ–º–æ

        # –•—Ä–∞–Ω–∏–ª–∏—â–µ –æ–±—É—á–∞—é—â–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ (–≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ - –±–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö)
        self.learning_examples: Dict[str, LearningExample] = {}
        self.last_retraining = None

        logger.info("Learning Pipeline Service initialized")

    async def process_rfc_completion(
        self, rfc: GeneratedRFC, session: GenerationSession
    ) -> Dict[str, Any]:
        """
        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ RFC –∏ –∑–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å —Å–±–æ—Ä–∞ —Ñ–∏–¥–±–µ–∫–∞.

        –í—ã–∑—ã–≤–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ RFC.
        """

        logger.info(f"Processing RFC completion for {rfc.id}")

        # 1. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        auto_quality_score = await self._auto_evaluate_rfc_quality(rfc, session)

        # 2. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        learning_data = await self._prepare_learning_data(
            rfc, session, auto_quality_score
        )

        # 3. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
        examples_count = len(self.learning_examples)
        needs_retraining = examples_count >= self.min_examples_for_retraining

        return {
            "rfc_id": rfc.id,
            "auto_quality_score": auto_quality_score,
            "learning_data_prepared": True,
            "examples_collected": examples_count,
            "needs_retraining": needs_retraining,
            "message": "RFC –≥–æ—Ç–æ–≤! –í–∞—à —Ñ–∏–¥–±–µ–∫ –ø–æ–º–æ–∂–µ—Ç —É–ª—É—á—à–∏—Ç—å —Å–∏—Å—Ç–µ–º—É.",
        }

    async def collect_user_feedback(
        self,
        rfc_id: str,
        feedback_type: FeedbackType,
        rating: Optional[int] = None,
        comment: Optional[str] = None,
    ) -> Dict[str, Any]:
        """–°–æ–±–∏—Ä–∞–µ—Ç —Ñ–∏–¥–±–µ–∫ –æ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç –æ–±—É—á–∞—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç."""

        logger.info(f"Collecting user feedback for RFC {rfc_id}: {feedback_type.value}")

        # 1. –û–±–Ω–æ–≤–ª—è–µ–º –æ–±—É—á–∞—é—â–∏–π –ø—Ä–∏–º–µ—Ä
        await self._update_learning_example_with_feedback(
            rfc_id, feedback_type, rating, comment
        )

        # 2. –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω—É–∂–Ω–æ –ª–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
        examples_count = len(self.learning_examples)

        retraining_result = None
        if examples_count >= self.min_examples_for_retraining:
            retraining_result = await self._trigger_retraining()

        return {
            "rfc_id": rfc_id,
            "feedback_collected": True,
            "examples_total": examples_count,
            "retraining_triggered": retraining_result is not None,
            "retraining_result": retraining_result,
            "message": f"–°–ø–∞—Å–∏–±–æ –∑–∞ —Ñ–∏–¥–±–µ–∫! –°–æ–±—Ä–∞–Ω–æ {examples_count} –ø—Ä–∏–º–µ—Ä–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è.",
        }

    async def _auto_evaluate_rfc_quality(
        self, rfc: GeneratedRFC, session: GenerationSession
    ) -> float:
        """–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ RFC —Å –ø–æ–º–æ—â—å—é LLM."""

        try:
            evaluation_prompt = f"""
–û—Ü–µ–Ω–∏ –∫–∞—á–µ—Å—Ç–≤–æ RFC –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ —à–∫–∞–ª–µ –æ—Ç 0.0 –¥–æ 1.0.

–ó–∞–ø—Ä–æ—Å: {session.initial_request}
–ó–∞–≥–æ–ª–æ–≤–æ–∫: {rfc.title}
–î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {len(rfc.full_content) if rfc.full_content else 0} —Å–∏–º–≤–æ–ª–æ–≤

–ö—Ä–∏—Ç–µ—Ä–∏–∏: —Å—Ç—Ä—É–∫—Ç—É—Ä–∞, —è—Å–Ω–æ—Å—Ç—å, –ø–æ–ª–Ω–æ—Ç–∞, –ø—Ä–∏–º–µ–Ω–∏–º–æ—Å—Ç—å.
–û—Ç–≤–µ—Ç—å —Ç–æ–ª—å–∫–æ —á–∏—Å–ª–æ–º –æ—Ç 0.0 –¥–æ 1.0.
"""

            response = await self.llm.generate(evaluation_prompt)
            score = float(response.strip())
            score = max(0.0, min(1.0, score))

            logger.info(f"Auto-evaluated RFC {rfc.id} quality: {score}")
            return score

        except Exception as e:
            logger.warning(f"Auto-evaluation failed: {e}")
            return 0.6  # –ù–µ–π—Ç—Ä–∞–ª—å–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –ø—Ä–∏ –æ—à–∏–±–∫–µ

    async def _prepare_learning_data(
        self, rfc: GeneratedRFC, session: GenerationSession, auto_quality_score: float
    ) -> LearningExample:
        """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è."""

        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        answers_text = "\n".join(
            [
                f"Q: {q.question}\nA: {next((a.answer for a in session.answers if a.question_id == q.id), '–ù–µ—Ç –æ—Ç–≤–µ—Ç–∞')}"
                for q in session.questions
            ]
        )

        input_context = f"""
Task: {session.task_type.value}
Request: {session.initial_request}

Q&A:
{answers_text}
""".strip()

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
        if auto_quality_score >= 0.8:
            quality = LearningQuality.EXCELLENT
        elif auto_quality_score >= 0.6:
            quality = LearningQuality.GOOD
        else:
            quality = LearningQuality.POOR

        learning_example = LearningExample(
            session_id=session.id,
            rfc_id=rfc.id,
            input_context=input_context,
            generated_content=rfc.full_content or "",
            feedback_score=auto_quality_score,
            quality=quality,
            user_comments=[],
            improvement_suggestions="",
            created_at=datetime.now(),
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ
        self.learning_examples[rfc.id] = learning_example

        logger.info(
            f"Prepared learning data for RFC {rfc.id} with quality {quality.value}"
        )
        return learning_example

    async def _update_learning_example_with_feedback(
        self,
        rfc_id: str,
        feedback_type: FeedbackType,
        rating: Optional[int] = None,
        comment: Optional[str] = None,
    ) -> None:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –æ–±—É—á–∞—é—â–∏–π –ø—Ä–∏–º–µ—Ä —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º —Ñ–∏–¥–±–µ–∫–æ–º."""

        learning_example = self.learning_examples.get(rfc_id)
        if not learning_example:
            logger.warning(f"Learning example not found for RFC {rfc_id}")
            return

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Ñ–∏–¥–±–µ–∫ –≤ —á–∏—Å–ª–æ–≤—É—é –æ—Ü–µ–Ω–∫—É
        if feedback_type == FeedbackType.LIKE:
            user_score = 0.8
        elif feedback_type == FeedbackType.DISLIKE:
            user_score = 0.2
        else:
            user_score = 0.5

        if rating:
            user_score = rating / 5.0  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –∫ 0-1

        # –û–±—ä–µ–¥–∏–Ω—è–µ–º –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫—É—é –∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫—É—é –æ—Ü–µ–Ω–∫–∏
        learning_example.feedback_score = (
            learning_example.feedback_score + user_score
        ) / 2

        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–∞—á–µ—Å—Ç–≤–æ
        if learning_example.feedback_score >= 0.8:
            learning_example.quality = LearningQuality.EXCELLENT
        elif learning_example.feedback_score >= 0.6:
            learning_example.quality = LearningQuality.GOOD
        else:
            learning_example.quality = LearningQuality.POOR

        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏
        if comment:
            learning_example.user_comments.append(comment)

        logger.info(f"Updated learning example for RFC {rfc_id} with user feedback")

    async def _trigger_retraining(self) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫–∞–µ—Ç –ø—Ä–æ—Ü–µ—Å—Å –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏."""

        logger.info("üß† Triggering model retraining based on collected feedback...")

        # 1. –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        training_examples = [
            ex
            for ex in self.learning_examples.values()
            if ex.quality in [LearningQuality.EXCELLENT, LearningQuality.GOOD]
        ]

        # 2. –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á—É –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        import uuid

        job_id = str(uuid.uuid4())

        # 3. –°–∏–º—É–ª–∏—Ä—É–µ–º –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ (–≤ —Ä–µ–∞–ª—å–Ω–æ—Å—Ç–∏ - –¥–ª–∏—Ç–µ–ª—å–Ω—ã–π –ø—Ä–æ—Ü–µ—Å—Å)
        await asyncio.sleep(2)  # –°–∏–º—É–ª—è—Ü–∏—è

        # 4. –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        self.last_retraining = datetime.now()

        logger.info(
            f"‚úÖ Model retraining completed! Used {len(training_examples)} quality examples"
        )

        return {
            "job_id": job_id,
            "training_examples": len(training_examples),
            "total_examples": len(self.learning_examples),
            "quality_examples": len(training_examples),
            "status": "completed",
            "improvement_estimate": "5-10% –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏",
            "message": "–ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤–∞—à–µ–≥–æ —Ñ–∏–¥–±–µ–∫–∞!",
        }

    async def get_learning_stats(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—É—á–µ–Ω–∏—è."""

        examples = list(self.learning_examples.values())

        if not examples:
            return {
                "total_examples": 0,
                "average_quality": 0.0,
                "quality_distribution": {},
                "ready_for_retraining": False,
            }

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞
        quality_counts = {}
        for quality in LearningQuality:
            count = sum(1 for ex in examples if ex.quality == quality)
            quality_counts[quality.value] = count

        average_quality = sum(ex.feedback_score for ex in examples) / len(examples)

        return {
            "total_examples": len(examples),
            "average_quality": round(average_quality, 2),
            "quality_distribution": quality_counts,
            "ready_for_retraining": len(examples) >= self.min_examples_for_retraining,
            "last_retraining": (
                self.last_retraining.isoformat() if self.last_retraining else None
            ),
        }


# Global instance
_learning_service_instance = None


def get_learning_service() -> LearningPipelineService:
    """Dependency injection –¥–ª—è learning service."""
    global _learning_service_instance
    if _learning_service_instance is None:
        _learning_service_instance = LearningPipelineService()
    return _learning_service_instance
