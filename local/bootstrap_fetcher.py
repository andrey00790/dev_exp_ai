#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
bootstrap_fetcher.py

–ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –æ–±—É—á–∞—é—â–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∏–∑ —Ä–∞–∑–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤.

–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:
- –ß–∏—Ç–∞–µ—Ç resource_config.yml
- –°–∫–∞—á–∏–≤–∞–µ—Ç –º–∞—Ç–µ—Ä–∏–∞–ª—ã –∏–∑ GitHub, PDF, –≤–µ–±-—Ä–µ—Å—É—Ä—Å–æ–≤
- –†–∞–∑–∞—Ä—Ö–∏–≤–∏—Ä—É–µ—Ç zip —Ñ–∞–π–ª—ã –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤ local/bootstrap
- –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
- –ò–Ω—Ç–µ–≥—Ä–∏—Ä—É–µ—Ç—Å—è —Å ingestion pipeline
"""

import os
import sys
import yaml
import json
import requests
import hashlib
import logging
import argparse
import tempfile
import shutil
import zipfile
from pathlib import Path
from typing import Dict, List, Any, Optional
from urllib.parse import urlparse, urljoin
from datetime import datetime
import time

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger("bootstrap_fetcher")

class BootstrapFetcher:
    """–ö–ª–∞—Å—Å –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –æ–±—É—á–∞—é—â–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –∏–∑ resource_config.yml"""
    
    def __init__(self, config_path: str = "resource_config.yml", output_dir: str = "bootstrap"):
        self.config_path = Path(config_path)
        self.output_dir = Path(output_dir)
        self.session = self._create_session()
        self.stats = {
            "total": 0,
            "downloaded": 0,
            "skipped": 0,
            "failed": 0,
            "sources": {}
        }
        
    def _create_session(self) -> requests.Session:
        """–°–æ–∑–¥–∞–µ—Ç HTTP —Å–µ—Å—Å–∏—é —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ retry –∏ user-agent"""
        session = requests.Session()
        session.headers.update({
            'User-Agent': 'AI-Assistant-Bootstrap-Fetcher/1.0 (Educational Material Downloader)'
        })
        return session
        
    def load_config(self) -> List[Dict[str, Any]]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é —Ä–µ—Å—É—Ä—Å–æ–≤ –∏–∑ YAML —Ñ–∞–π–ª–∞"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                content = f.read()
                # –ò—Å–ø—Ä–∞–≤–ª—è–µ–º –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –≤ YAML
                content = self._fix_yaml_syntax(content)
                config = yaml.safe_load(content)
                
            resources = config.get('resources', [])
            logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {len(resources)} —Ä–µ—Å—É—Ä—Å–æ–≤")
            return resources
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ {self.config_path}: {e}")
            return []
            
    def _fix_yaml_syntax(self, content: str) -> str:
        """–ò—Å–ø—Ä–∞–≤–ª—è–µ—Ç –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã –≤ YAML —Ñ–∞–π–ª–µ"""
        # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ —Å—Ç—Ä–æ–∫–∏ —Å —Å–∏–º–≤–æ–ª–∞–º–∏ ^[ –∏ attribution
        lines = content.split('\n')
        fixed_lines = []
        
        for line in lines:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–º–∏ —Å–∏–º–≤–æ–ª–∞–º–∏
            if '^[' in line or 'attribution' in line or ']({"' in line:
                continue
            fixed_lines.append(line)
            
        return '\n'.join(fixed_lines)
        
    def categorize_url(self, url: str) -> Dict[str, str]:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Ç–∏–ø URL –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∑–∞–≥—Ä—É–∑–∫–∏"""
        parsed = urlparse(url)
        domain = parsed.netloc.lower()
        path = parsed.path.lower()
        
        if 'github.com' in domain:
            if '/blob/' in path or '/raw/' in path:
                return {"type": "github_file", "strategy": "direct"}
            else:
                return {"type": "github_repo", "strategy": "clone_or_archive"}
                
        elif path.endswith('.pdf'):
            return {"type": "pdf", "strategy": "direct"}
            
        elif any(ext in path for ext in ['.md', '.txt', '.py', '.js', '.go', '.rs']):
            return {"type": "text_file", "strategy": "direct"}
            
        elif 'harvard.edu' in domain or 'edx.org' in domain:
            return {"type": "educational", "strategy": "webpage"}
            
        else:
            return {"type": "webpage", "strategy": "webpage"}
            
    def generate_filename(self, resource: Dict[str, Any], url_info: Dict[str, str]) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∏–º—è —Ñ–∞–π–ª–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ—Å—É—Ä—Å–∞"""
        name = resource.get('name', 'unknown')
        category = resource.get('category', 'general')
        url_type = url_info.get('type', 'file')
        
        # –û—á–∏—â–∞–µ–º –∏–º—è –æ—Ç –Ω–µ–¥–æ–ø—É—Å—Ç–∏–º—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤
        safe_name = "".join(c for c in name if c.isalnum() or c in (' ', '-', '_')).strip()
        safe_name = safe_name.replace(' ', '_')
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ
        if url_type == 'pdf':
            ext = '.pdf'
        elif url_type == 'github_repo':
            # –î–ª—è GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤ –º—ã —Å–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É, –∞ –Ω–µ —Ñ–∞–π–ª
            return safe_name
        elif url_type in ['text_file', 'webpage']:
            ext = '.txt'
        else:
            ext = '.txt'
            
        return f"{category}_{safe_name}{ext}"

    def extract_archive(self, archive_path: Path, extract_to: Path) -> bool:
        """–†–∞–∑–∞—Ä—Ö–∏–≤–∏—Ä—É–µ—Ç –∞—Ä—Ö–∏–≤ –≤ —É–∫–∞–∑–∞–Ω–Ω—É—é –ø–∞–ø–∫—É"""
        try:
            extract_to.mkdir(parents=True, exist_ok=True)
            
            if archive_path.suffix.lower() == '.zip':
                with zipfile.ZipFile(archive_path, 'r') as zip_ref:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –≤—Å–µ —Ñ–∞–π–ª—ã
                    zip_ref.extractall(extract_to)
                    
                    # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
                    extracted_files = []
                    for root, dirs, files in os.walk(extract_to):
                        for file in files:
                            extracted_files.append(os.path.join(root, file))
                    
                    logger.info(f"–†–∞–∑–∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–æ {len(extracted_files)} —Ñ–∞–π–ª–æ–≤ –≤ {extract_to}")
                    return True
            else:
                logger.warning(f"–ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç –∞—Ä—Ö–∏–≤–∞: {archive_path.suffix}")
                return False
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏–∏ {archive_path}: {e}")
            return False
        
    def download_github_repo(self, url: str, output_path: Path) -> bool:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –∏ —Ä–∞–∑–∞—Ä—Ö–∏–≤–∏—Ä—É–µ—Ç –µ–≥–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ"""
        try:
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º URL –≤ —Å—Å—ã–ª–∫—É –Ω–∞ –∞—Ä—Ö–∏–≤
            if url.endswith('.git'):
                url = url[:-4]
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º owner/repo –∏–∑ URL
            parts = url.replace('https://github.com/', '').split('/')
            if len(parts) >= 2:
                owner, repo = parts[0], parts[1]
                archive_url = f"https://github.com/{owner}/{repo}/archive/refs/heads/main.zip"
                
                # –ü—Ä–æ–±—É–µ–º main, –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏—Ç—Å—è - –ø–æ–ø—Ä–æ–±—É–µ–º master
                response = self.session.get(archive_url, timeout=30)
                if response.status_code != 200:
                    archive_url = f"https://github.com/{owner}/{repo}/archive/refs/heads/master.zip"
                    response = self.session.get(archive_url, timeout=30)
                
                if response.status_code == 200:
                    # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è zip –∞—Ä—Ö–∏–≤–∞
                    with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as tmp_file:
                        tmp_file.write(response.content)
                        tmp_path = Path(tmp_file.name)
                    
                    try:
                        # –†–∞–∑–∞—Ä—Ö–∏–≤–∏—Ä—É–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—É—é –ø–∞–ø–∫—É
                        with tempfile.TemporaryDirectory() as tmp_dir:
                            tmp_extract_path = Path(tmp_dir)
                            
                            if self.extract_archive(tmp_path, tmp_extract_path):
                                # –ù–∞—Ö–æ–¥–∏–º –∫–æ—Ä–Ω–µ–≤—É—é –ø–∞–ø–∫—É —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è (–æ–±—ã—á–Ω–æ –∏–º–µ–µ—Ç –Ω–∞–∑–≤–∞–Ω–∏–µ –≤–∏–¥–∞ repo-main –∏–ª–∏ repo-master)
                                repo_folders = [d for d in tmp_extract_path.iterdir() if d.is_dir()]
                                
                                if repo_folders:
                                    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –ø–∞–ø–∫—É (–æ–±—ã—á–Ω–æ –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è)
                                    source_folder = repo_folders[0]
                                    
                                    # –°–æ–∑–¥–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–∞–ø–∫—É
                                    output_path.mkdir(parents=True, exist_ok=True)
                                    
                                    # –ö–æ–ø–∏—Ä—É–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ
                                    for item in source_folder.iterdir():
                                        dest_item = output_path / item.name
                                        if item.is_dir():
                                            shutil.copytree(item, dest_item, dirs_exist_ok=True)
                                        else:
                                            shutil.copy2(item, dest_item)
                                    
                                    logger.info(f"–†–∞–∑–∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π: {url} -> {output_path}")
                                    return True
                                else:
                                    logger.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –≤ –∞—Ä—Ö–∏–≤–µ {url}")
                            else:
                                logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞–∑–∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏—è {url}")
                                
                    finally:
                        # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π zip —Ñ–∞–π–ª
                        tmp_path.unlink(missing_ok=True)
                        
                else:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–∞—á–∞—Ç—å —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π {url}: HTTP {response.status_code}")
                    
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ GitHub —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è {url}: {e}")
            
        return False
        
    def download_direct_file(self, url: str, output_path: Path) -> bool:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç —Ñ–∞–π–ª –Ω–∞–ø—Ä—è–º—É—é –∏ —Ä–∞–∑–∞—Ä—Ö–∏–≤–∏—Ä—É–µ—Ç –µ—Å–ª–∏ —ç—Ç–æ –∞—Ä—Ö–∏–≤"""
        try:
            response = self.session.get(url, timeout=60, stream=True)
            response.raise_for_status()
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ñ–∞–π–ª –∞—Ä—Ö–∏–≤–æ–º –ø–æ URL –∏–ª–∏ Content-Type
            is_archive = (
                url.lower().endswith('.zip') or 
                'zip' in response.headers.get('content-type', '').lower() or
                'application/zip' in response.headers.get('content-type', '').lower()
            )
            
            if is_archive:
                # –°–∫–∞—á–∏–≤–∞–µ–º –∞—Ä—Ö–∏–≤ –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                with tempfile.NamedTemporaryFile(suffix='.zip', delete=False) as tmp_file:
                    for chunk in response.iter_content(chunk_size=8192):
                        tmp_file.write(chunk)
                    tmp_path = Path(tmp_file.name)
                
                try:
                    # –î–ª—è –∞—Ä—Ö–∏–≤–æ–≤ —Å–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –∏ —Ä–∞–∑–∞—Ä—Ö–∏–≤–∏—Ä—É–µ–º –≤ –Ω–µ—ë
                    if output_path.suffix:
                        # –£–±–∏—Ä–∞–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–∏–µ –∏ –¥–µ–ª–∞–µ–º –ø–∞–ø–∫–æ–π
                        output_path = output_path.with_suffix('')
                    
                    output_path.mkdir(parents=True, exist_ok=True)
                    
                    success = self.extract_archive(tmp_path, output_path)
                    if success:
                        logger.info(f"–°–∫–∞—á–∞–Ω –∏ —Ä–∞–∑–∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω —Ñ–∞–π–ª: {url} -> {output_path}")
                    else:
                        logger.error(f"–û—à–∏–±–∫–∞ —Ä–∞–∑–∞—Ä—Ö–∏–≤–∏—Ä–æ–≤–∞–Ω–∏—è —Ñ–∞–π–ª–∞: {url}")
                    return success
                    
                finally:
                    # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                    tmp_path.unlink(missing_ok=True)
            else:
                # –û–±—ã—á–Ω—ã–π —Ñ–∞–π–ª, —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                with open(output_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)
                        
                logger.info(f"–°–∫–∞—á–∞–Ω —Ñ–∞–π–ª: {url}")
                return True
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ —Ñ–∞–π–ª–∞ {url}: {e}")
            return False
            
    def download_webpage(self, url: str, output_path: Path) -> bool:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—É –∫–∞–∫ —Ç–µ–∫—Å—Ç"""
        try:
            response = self.session.get(url, timeout=30)
            response.raise_for_status()
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫–∞–∫ —Ç–µ–∫—Å—Ç —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
            content = f"# {url}\n"
            content += f"# Downloaded: {datetime.now().isoformat()}\n"
            content += f"# Content-Type: {response.headers.get('content-type', 'unknown')}\n\n"
            
            if 'text/' in response.headers.get('content-type', ''):
                content += response.text
            else:
                content += f"Binary content ({len(response.content)} bytes)\n"
                content += f"Note: This was a binary file, only metadata saved.\n"
                
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(content)
                
            logger.info(f"–°–∫–∞—á–∞–Ω–∞ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü–∞: {url}")
            return True
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–∫–∞—á–∏–≤–∞–Ω–∏–∏ –≤–µ–±-—Å—Ç—Ä–∞–Ω–∏—Ü—ã {url}: {e}")
            return False
            
    def download_resource(self, resource: Dict[str, Any]) -> bool:
        """–°–∫–∞—á–∏–≤–∞–µ—Ç –æ–¥–∏–Ω —Ä–µ—Å—É—Ä—Å"""
        url = resource.get('url')
        if not url:
            logger.warning(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç URL –¥–ª—è —Ä–µ—Å—É—Ä—Å–∞: {resource.get('name')}")
            return False
            
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø —Ä–µ—Å—É—Ä—Å–∞ –∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∑–∞–≥—Ä—É–∑–∫–∏
        url_info = self.categorize_url(url)
        filename = self.generate_filename(resource, url_info)
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏
        category = resource.get('category', 'general')
        category_dir = self.output_dir / category
        category_dir.mkdir(parents=True, exist_ok=True)
        
        output_path = category_dir / filename
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –ª–∏ —É–∂–µ —Ñ–∞–π–ª
        if output_path.exists():
            logger.info(f"–§–∞–π–ª —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º: {output_path}")
            self.stats["skipped"] += 1
            return True
            
        # –í—ã–±–∏—Ä–∞–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é –∑–∞–≥—Ä—É–∑–∫–∏
        strategy = url_info.get('strategy')
        success = False
        
        if strategy == 'clone_or_archive':
            success = self.download_github_repo(url, output_path)
        elif strategy == 'direct':
            success = self.download_direct_file(url, output_path)
        elif strategy == 'webpage':
            success = self.download_webpage(url, output_path)
        else:
            logger.warning(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –∑–∞–≥—Ä—É–∑–∫–∏: {strategy}")
            
        if success:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            self.save_metadata(resource, output_path, url_info)
            self.stats["downloaded"] += 1
        else:
            self.stats["failed"] += 1
            
        return success
        
    def save_metadata(self, resource: Dict[str, Any], file_path: Path, url_info: Dict[str, str]):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Ä–µ—Å—É—Ä—Å–∞"""
        metadata = {
            "name": resource.get('name'),
            "role": resource.get('role'),
            "category": resource.get('category'),
            "url": resource.get('url'),
            "downloaded_at": datetime.now().isoformat(),
            "file_path": str(file_path),
            "url_type": url_info.get('type'),
            "strategy": url_info.get('strategy'),
            "file_size": file_path.stat().st_size if file_path.exists() else 0
        }
        
        metadata_path = file_path.with_suffix('.json')
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
            
    def update_source_stats(self, category: str):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞–º"""
        if category not in self.stats["sources"]:
            self.stats["sources"][category] = {"count": 0, "downloaded": 0}
        self.stats["sources"][category]["count"] += 1
        
    def fetch_all(self, max_resources: Optional[int] = None) -> Dict[str, Any]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Ä–µ—Å—É—Ä—Å—ã –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        logger.info("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –æ–±—É—á–∞—é—â–∏—Ö –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤...")
        
        # –°–æ–∑–¥–∞–µ–º –≤—ã—Ö–æ–¥–Ω—É—é –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
        resources = self.load_config()
        if not resources:
            logger.error("–ù–µ—Ç —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏")
            return self.stats
            
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ—Å—É—Ä—Å–æ–≤ –µ—Å–ª–∏ –∑–∞–¥–∞–Ω–æ
        if max_resources:
            resources = resources[:max_resources]
            
        self.stats["total"] = len(resources)
        logger.info(f"üìä –í—Å–µ–≥–æ —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏: {len(resources)}")
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ—Å—É—Ä—Å—ã
        for i, resource in enumerate(resources, 1):
            name = resource.get('name', 'Unknown')
            category = resource.get('category', 'general')
            
            logger.info(f"üì• [{i}/{len(resources)}] –ó–∞–≥—Ä—É–∂–∞–µ–º: {name} ({category})")
            self.update_source_stats(category)
            
            try:
                success = self.download_resource(resource)
                if success:
                    self.stats["sources"][category]["downloaded"] += 1
                    
            except Exception as e:
                logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ {name}: {e}")
                self.stats["failed"] += 1
                
            # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–≥—Ä—É–∑–∫–∞–º–∏
            time.sleep(0.5)
            
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—â—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self.save_final_stats()
        
        logger.info("‚úÖ –ó–∞–≥—Ä—É–∑–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        self.print_stats()
        
        return self.stats
        
    def save_final_stats(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        stats_path = self.output_dir / "download_stats.json"
        with open(stats_path, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, indent=2, ensure_ascii=False)
            
    def print_stats(self):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∑–∞–≥—Ä—É–∑–∫–∏"""
        print("\n" + "="*60)
        print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ó–ê–ì–†–£–ó–ö–ò –ú–ê–¢–ï–†–ò–ê–õ–û–í")
        print("="*60)
        print(f"–í—Å–µ–≥–æ —Ä–µ—Å—É—Ä—Å–æ–≤: {self.stats['total']}")
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ: {self.stats['downloaded']}")
        print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ (—É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç): {self.stats['skipped']}")
        print(f"–û—à–∏–±–∫–∏: {self.stats['failed']}")
        print(f"–£—Å–ø–µ—à–Ω–æ—Å—Ç—å: {(self.stats['downloaded'] + self.stats['skipped']) / self.stats['total'] * 100:.1f}%")
        
        print("\nüìÅ –ü–û –ö–ê–¢–ï–ì–û–†–ò–Ø–ú:")
        for category, stats in self.stats["sources"].items():
            print(f"  {category}: {stats['downloaded']}/{stats['count']}")
        print("="*60)


def main():
    parser = argparse.ArgumentParser(description="Bootstrap –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ –¥–ª—è AI Assistant")
    parser.add_argument(
        "--config", 
        default="resource_config.yml",
        help="–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: resource_config.yml)"
    )
    parser.add_argument(
        "--output", 
        default="bootstrap",
        help="–ü–∞–ø–∫–∞ –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–∞—Ç–µ—Ä–∏–∞–ª–æ–≤ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: bootstrap)"
    )
    parser.add_argument(
        "--max-resources", 
        type=int,
        help="–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ—Å—É—Ä—Å–æ–≤ –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ (–¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)"
    )
    parser.add_argument(
        "--category", 
        help="–ó–∞–≥—Ä—É–∂–∞—Ç—å —Ç–æ–ª—å–∫–æ —Ä–µ—Å—É—Ä—Å—ã –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏"
    )
    
    args = parser.parse_args()
    
    try:
        fetcher = BootstrapFetcher(
            config_path=args.config,
            output_dir=args.output
        )
        
        stats = fetcher.fetch_all(max_resources=args.max_resources)
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–æ–¥ –≤—ã—Ö–æ–¥–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if stats["failed"] == 0:
            sys.exit(0)
        elif stats["downloaded"] > 0:
            sys.exit(1)  # –ß–∞—Å—Ç–∏—á–Ω—ã–π —É—Å–ø–µ—Ö
        else:
            sys.exit(2)  # –ü–æ–ª–Ω–∞—è –Ω–µ—É–¥–∞—á–∞
            
    except KeyboardInterrupt:
        logger.info("‚ùå –ó–∞–≥—Ä—É–∑–∫–∞ –ø—Ä–µ—Ä–≤–∞–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        sys.exit(130)
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main() 