.PHONY: up down test healthcheck run bootstrap smoke-test

docker_available := $(shell command -v docker 2>/dev/null)

# Bootstrap - –ø–æ–ª–Ω–æ–µ —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ –ø—Ä–æ–µ–∫—Ç–∞ –æ–¥–Ω–æ–π –∫–æ–º–∞–Ω–¥–æ–π
bootstrap:
	@echo "üöÄ Bootstrapping AI Assistant MVP..."
	@if [ ! -f .env.local ]; then \
		echo "üìã Creating .env.local from .env.example..."; \
		cp .env.example .env.local; \
	fi
	@echo "üì¶ Installing dependencies..."
	@pip3 install -r requirements.txt
	@echo "üê≥ Starting infrastructure..."
	@$(MAKE) up
	@echo "‚è≥ Waiting for services to be ready..."
	@sleep 10
	@echo "üîç Running health checks..."
	@$(MAKE) healthcheck
	@echo "üß™ Running tests..."
	@$(MAKE) test
	@echo "üí® Running smoke tests..."
	@$(MAKE) smoke-test
	@echo "‚úÖ Bootstrap complete! AI Assistant MVP is ready."
	@echo "üåê Application: http://localhost:8000"
	@echo "üìä API Docs: http://localhost:8000/docs"
	@echo "üîç Qdrant: http://localhost:6333/dashboard"

up:
	@if [ -n "$(docker_available)" ]; then \
		echo "üê≥ Starting Docker services..."; \
		docker compose up -d --build; \
		echo "‚è≥ Waiting for services to start..."; \
		sleep 5; \
	else \
		echo "‚ö†Ô∏è  Docker not available, starting locally..."; \
		PYTHONPATH=. nohup uvicorn app.main:app --host 0.0.0.0 --port 8000 > /dev/null 2>&1 & echo $$! > .app.pid; \
		echo "‚úÖ App started with PID $$(cat .app.pid)"; \
	fi

down:
	@if [ -n "$(docker_available)" ]; then \
		echo "üõë Stopping Docker services..."; \
		docker compose down; \
	else \
		if [ -f .app.pid ]; then \
			kill $$(cat .app.pid) && rm .app.pid && echo "‚úÖ App stopped"; \
		else \
			echo "‚ö†Ô∏è  No running app found"; \
		fi; \
	fi

healthcheck:
	@echo "üîç Checking service health..."
	@if ! curl -fs http://localhost:8000/health >/dev/null 2>&1; then \
		echo "‚ùå Service not responding, attempting to start..."; \
		PYTHONPATH=. uvicorn app.main:app --host 0.0.0.0 --port 8000 & \
		pid=$$!; \
		sleep 5; \
		if curl -f http://localhost:8000/health; then \
			echo "‚úÖ Health check passed"; \
		else \
			echo "‚ùå Health check failed"; \
			exit 1; \
		fi; \
		kill $$pid; \
	else \
		curl -f http://localhost:8000/health && echo "‚úÖ Service is healthy"; \
	fi

test:
	@echo "üß™ Running tests..."
	@PYTHONPATH=. python3 -m pytest -v --cov=app --cov-report=term-missing --cov-fail-under=80

smoke-test:
	@echo "üí® Running smoke tests..."
	@PYTHONPATH=. python3 -m pytest tests/smoke/ -v --tb=short

run:
	@echo "üöÄ Starting development server..."
	@PYTHONPATH=. uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload

# –û—á–∏—Å—Ç–∫–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö –∏ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–æ–≤
clean:
	@echo "üßπ Cleaning up..."
	@if [ -n "$(docker_available)" ]; then \
		docker compose down -v --remove-orphans; \
		docker system prune -f; \
	fi
	@rm -f .app.pid
	@echo "‚úÖ Cleanup complete"

# –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç—É—Å –≤—Å–µ—Ö —Å–µ—Ä–≤–∏—Å–æ–≤
status:
	@echo "üìä Service Status:"
	@echo "Web API: $$(curl -s http://localhost:8000/health > /dev/null && echo '‚úÖ UP' || echo '‚ùå DOWN')"
	@echo "Qdrant: $$(curl -s http://localhost:6333/dashboard > /dev/null && echo '‚úÖ UP' || echo '‚ùå DOWN')"
	@echo "Ollama: $$(curl -s http://localhost:11434/api/tags > /dev/null && echo '‚úÖ UP' || echo '‚ùå DOWN')"
	@if [ -n "$(docker_available)" ]; then \
		echo "Docker services:"; \
		docker compose ps; \
	fi

# =============================================================================
# E2E PIPELINE WITH MODEL TRAINING
# =============================================================================

.PHONY: e2e-full-pipeline
e2e-full-pipeline: ## –ü–æ–ª–Ω—ã–π E2E –ø–∞–π–ø–ª–∞–π–Ω —Å –æ–±—É—á–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–∏
	@echo "üöÄ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ E2E –ø–∞–π–ø–ª–∞–π–Ω–∞ —Å –æ–±—É—á–µ–Ω–∏–µ–º –º–æ–¥–µ–ª–∏..."
	docker-compose up -d
	@echo "‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–æ–≤ (3 –º–∏–Ω—É—Ç—ã)..."
	sleep 180
	python -m pytest test_e2e_extended.py::TestE2EExtendedPipeline::test_complete_e2e_pipeline_with_model_training -v -s --tb=short
	@echo "‚úÖ –ü–æ–ª–Ω—ã–π E2E –ø–∞–π–ø–ª–∞–π–Ω –∑–∞–≤–µ—Ä—à—ë–Ω"

.PHONY: train-model
train-model: ## –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ dataset_config.yml
	@echo "üß† –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏..."
	python model_training.py
	@echo "‚úÖ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ"

.PHONY: test-feedback
test-feedback: ## –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏
	@echo "üîÑ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏..."
	python -m pytest test_e2e_extended.py::TestE2EExtendedPipeline::test_feedback_and_retraining -v -s
	@echo "‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ"

.PHONY: test-multilingual
test-multilingual: ## –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
	@echo "üåê –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏..."
	python -m pytest test_e2e_extended.py::TestE2EExtendedPipeline::test_multilingual_model_performance -v -s
	@echo "‚úÖ –ú—É–ª—å—Ç–∏—è–∑—ã—á–Ω–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ"

.PHONY: run-semantic-search-tests
run-semantic-search-tests: ## –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞
	@echo "üîç –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞..."
	python evaluate_semantic_search.py --language all --output results/semantic_search_results.json
	@echo "‚úÖ –¢–µ—Å—Ç—ã —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–æ–∏—Å–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω—ã"

.PHONY: run-rfc-tests
run-rfc-tests: ## –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ RFC –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
	@echo "üìù –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ RFC –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏..."
	python validate_rfc.py --language all --output results/rfc_validation_results.json
	@echo "‚úÖ –¢–µ—Å—Ç—ã RFC –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω—ã"

.PHONY: check-model-quality
check-model-quality: ## –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –≤ PostgreSQL
	@echo "üìä –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏..."
	python -c "
import psycopg2
import json
import os
from datetime import datetime, timedelta

conn = psycopg2.connect(
    host=os.getenv('POSTGRES_HOST', 'localhost'),
    port=os.getenv('POSTGRES_PORT', 5432),
    database=os.getenv('POSTGRES_DB', 'testdb'),
    user=os.getenv('POSTGRES_USER', 'testuser'),
    password=os.getenv('POSTGRES_PASSWORD', 'testpass')
)

with conn.cursor() as cursor:
    cursor.execute('''
        SELECT metric_name, AVG(metric_value) as avg_value, COUNT(*) as count
        FROM model_metrics 
        WHERE timestamp > %s
        GROUP BY metric_name
        ORDER BY metric_name
    ''', (datetime.now() - timedelta(days=7),))
    
    results = cursor.fetchall()
    
    print('üìà –ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–∏ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω—é—é –Ω–µ–¥–µ–ª—é:')
    for metric_name, avg_value, count in results:
        print(f'  {metric_name}: {avg_value:.3f} (–∑–∞–ø–∏—Å–µ–π: {count})')
        
    cursor.execute('''
        SELECT COUNT(*) as feedback_count
        FROM model_feedback 
        WHERE timestamp > %s
    ''', (datetime.now() - timedelta(days=7),))
    
    feedback_count = cursor.fetchone()[0]
    print(f'üí¨ –ó–∞–ø–∏—Å–µ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏: {feedback_count}')

conn.close()
"
	@echo "‚úÖ –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞"

.PHONY: simulate-user-feedback
simulate-user-feedback: ## –°–∏–º—É–ª—è—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏
	@echo "üë• –°–∏–º—É–ª—è—Ü–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏..."
	python -c "
import psycopg2
import json
import os
import random
from datetime import datetime

feedback_scenarios = [
    {'query': 'OAuth 2.0 implementation', 'doc': 'OAuth Guide', 'score': 0.95, 'lang': 'en'},
    {'query': '—Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è OAuth 2.0', 'doc': '–†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ OAuth', 'score': 0.93, 'lang': 'ru'},
    {'query': 'microservices patterns', 'doc': 'Microservices Guide', 'score': 0.88, 'lang': 'en'},
    {'query': '–ø–∞—Ç—Ç–µ—Ä–Ω—ã –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–æ–≤', 'doc': '–†—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ –º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å–∞–º', 'score': 0.86, 'lang': 'ru'},
]

conn = psycopg2.connect(
    host=os.getenv('POSTGRES_HOST', 'localhost'),
    port=os.getenv('POSTGRES_PORT', 5432),
    database=os.getenv('POSTGRES_DB', 'testdb'),
    user=os.getenv('POSTGRES_USER', 'testuser'),
    password=os.getenv('POSTGRES_PASSWORD', 'testpass')
)

with conn.cursor() as cursor:
    cursor.execute('''
        CREATE TABLE IF NOT EXISTS model_feedback (
            id SERIAL PRIMARY KEY,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            query TEXT NOT NULL,
            document TEXT NOT NULL,
            relevance_score FLOAT NOT NULL,
            language VARCHAR(10),
            user_id VARCHAR(100),
            processed BOOLEAN DEFAULT FALSE,
            metadata JSONB
        )
    ''')
    
    for i in range(20):
        scenario = random.choice(feedback_scenarios)
        cursor.execute('''
            INSERT INTO model_feedback 
            (query, document, relevance_score, language, user_id, metadata)
            VALUES (%s, %s, %s, %s, %s, %s)
        ''', (
            scenario['query'],
            scenario['doc'],
            scenario['score'] + random.uniform(-0.1, 0.1),
            scenario['lang'],
            f'sim_user_{i}',
            json.dumps({'simulated': True, 'batch': datetime.now().isoformat()})
        ))
    
    conn.commit()
    
print('‚úÖ –°–æ–∑–¥–∞–Ω–æ 20 –∑–∞–ø–∏—Å–µ–π —Å–∏–º—É–ª–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏')
conn.close()
"
	@echo "‚úÖ –°–∏–º—É–ª—è—Ü–∏—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞"

.PHONY: retrain-model
retrain-model: ## –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏
	@echo "üîÑ –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏..."
	python -c "
from model_training import ModelTrainer

trainer = ModelTrainer()
feedback_data = trainer.get_feedback_from_postgres()

if len(feedback_data) >= 10:
    print(f'üîÑ –ù–∞–π–¥–µ–Ω–æ {len(feedback_data)} –∑–∞–ø–∏—Å–µ–π –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏')
    print('üìö –ó–∞–ø—É—Å–∫ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è...')
    trainer.retrain_with_feedback(feedback_data[:50])  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    print('‚úÖ –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ')
    
    metrics = trainer.evaluate_model()
    print(f'üìä –ù–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏: {metrics}')
else:
    print(f'‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è: {len(feedback_data)} –∑–∞–ø–∏—Å–µ–π')
"
	@echo "‚úÖ –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ"

.PHONY: e2e-quick
e2e-quick: ## –ë—ã—Å—Ç—Ä—ã–π E2E —Ç–µ—Å—Ç (–±–µ–∑ –ø–æ–ª–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è)
	@echo "‚ö° –ë—ã—Å—Ç—Ä—ã–π E2E —Ç–µ—Å—Ç..."
	docker-compose up -d postgres elasticsearch redis
	@echo "‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –±–∞–∑–æ–≤—ã—Ö —Å–µ—Ä–≤–∏—Å–æ–≤ (30 —Å–µ–∫)..."
	sleep 30
	python -m pytest test_e2e_extended.py::TestSpecificE2EScenarios::test_concurrent_feedback_processing -v -s
	@echo "‚úÖ –ë—ã—Å—Ç—Ä—ã–π E2E —Ç–µ—Å—Ç –∑–∞–≤–µ—Ä—à—ë–Ω"

.PHONY: e2e-advanced-scenarios
e2e-advanced-scenarios: ## –ó–∞–ø—É—Å–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö E2E —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤
	@echo "üéØ –ó–∞–ø—É—Å–∫ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö E2E —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤..."
	python -m pytest test_e2e_extended.py::TestE2EExtendedPipeline::_run_advanced_e2e_scenarios -v -s
	@echo "‚úÖ –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ E2E —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω—ã"

.PHONY: monitor-model-performance
monitor-model-performance: ## –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏
	@echo "üìà –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏..."
	python -c "
import psycopg2
import json
import os
from datetime import datetime, timedelta
import matplotlib.pyplot as plt
import pandas as pd

conn = psycopg2.connect(
    host=os.getenv('POSTGRES_HOST', 'localhost'),
    port=os.getenv('POSTGRES_PORT', 5432),
    database=os.getenv('POSTGRES_DB', 'testdb'),
    user=os.getenv('POSTGRES_USER', 'testuser'),
    password=os.getenv('POSTGRES_PASSWORD', 'testpass')
)

# –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π
with conn.cursor() as cursor:
    cursor.execute('''
        SELECT 
            DATE(timestamp) as date,
            metric_name,
            AVG(metric_value) as avg_value
        FROM model_metrics 
        WHERE timestamp > %s
        GROUP BY DATE(timestamp), metric_name
        ORDER BY date DESC, metric_name
    ''', (datetime.now() - timedelta(days=30),))
    
    results = cursor.fetchall()
    
    if results:
        df = pd.DataFrame(results, columns=['date', 'metric', 'value'])
        print('üìä –¢—Ä–µ–Ω–¥—ã –º–µ—Ç—Ä–∏–∫ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π:')
        print(df.pivot(index='date', columns='metric', values='value'))
    else:
        print('‚ö†Ô∏è –ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ –º–µ—Ç—Ä–∏–∫–∞—Ö –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30 –¥–Ω–µ–π')

conn.close()
"
	@echo "‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∑–∞–≤–µ—Ä—à—ë–Ω"

# =============================================================================
# DATASET AND TRAINING MANAGEMENT
# =============================================================================

.PHONY: validate-dataset
validate-dataset: ## –í–∞–ª–∏–¥–∞—Ü–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞
	@echo "‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è dataset_config.yml..."
	python -c "
import yaml
import json

with open('dataset_config.yml', 'r', encoding='utf-8') as f:
    config = yaml.safe_load(f)

print('üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞:')
print(f'  –í–µ—Ä—Å–∏—è: {config['metadata']['version']}')
print(f'  –Ø–∑—ã–∫–∏: {config['metadata']['languages']}')
print(f'  –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {config['metadata']['total_documents']}')
print(f'  –ö–∞—Ç–µ–≥–æ—Ä–∏–∏: {config['metadata']['categories']}')

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö
sources = config['data_sources']
enabled_sources = [name for name, conf in sources.items() if conf.get('enabled', False)]
print(f'  –ê–∫—Ç–∏–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏: {enabled_sources}')

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—É—á–∞—é—â–∏–µ –ø–∞—Ä—ã
training_pairs = config.get('training_pairs', {}).get('semantic_search', [])
print(f'  –û–±—É—á–∞—é—â–∏—Ö –ø–∞—Ä: {len(training_pairs)}')

# –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏
model_config = config['model_config']
print(f'  –ú–æ–¥–µ–ª—å: {model_config['embeddings']['model_name']}')
print(f'  –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å: {model_config['embeddings']['dimensions']}')
print(f'  Batch size: {model_config['embeddings']['batch_size']}')

print('‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –≤–∞–ª–∏–¥–Ω–∞')
"
	@echo "‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞—Ç–∞—Å–µ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞"

.PHONY: backup-model
backup-model: ## –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏
	@echo "üíæ –°–æ–∑–¥–∞–Ω–∏–µ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏ –º–æ–¥–µ–ª–∏..."
	@if [ -d "./trained_model" ]; then \
		timestamp=$$(date +"%Y%m%d_%H%M%S"); \
		cp -r ./trained_model ./backups/model_backup_$$timestamp; \
		echo "‚úÖ –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è —Å–æ–∑–¥–∞–Ω–∞: ./backups/model_backup_$$timestamp"; \
	else \
		echo "‚ö†Ô∏è –û–±—É—á–µ–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ ./trained_model"; \
	fi

.PHONY: restore-model
restore-model: ## –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏
	@echo "üîÑ –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏–∑ —Ä–µ–∑–µ—Ä–≤–Ω–æ–π –∫–æ–ø–∏–∏..."
	@if [ -z "$(BACKUP)" ]; then \
		echo "‚ùå –£–∫–∞–∂–∏—Ç–µ —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é: make restore-model BACKUP=model_backup_20231201_120000"; \
		exit 1; \
	fi
	@if [ -d "./backups/$(BACKUP)" ]; then \
		rm -rf ./trained_model; \
		cp -r ./backups/$(BACKUP) ./trained_model; \
		echo "‚úÖ –ú–æ–¥–µ–ª—å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –∏–∑ $(BACKUP)"; \
	else \
		echo "‚ùå –†–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è ./backups/$(BACKUP) –Ω–µ –Ω–∞–π–¥–µ–Ω–∞"; \
		exit 1; \
	fi

.PHONY: clean-model-data
clean-model-data: ## –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç—Ä–∏–∫
	@echo "üßπ –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–∏..."
	@read -p "–í—ã —É–≤–µ—Ä–µ–Ω—ã? –≠—Ç–æ —É–¥–∞–ª–∏—Ç –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –∏ –º–µ—Ç—Ä–∏–∫–∏ (y/N): " confirm; \
	if [ "$$confirm" = "y" ] || [ "$$confirm" = "Y" ]; then \
		rm -rf ./trained_model ./model_output; \
		docker-compose exec postgres psql -U testuser -d testdb -c "TRUNCATE TABLE model_metrics, model_feedback;"; \
		echo "‚úÖ –î–∞–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏ –æ—á–∏—â–µ–Ω—ã"; \
	else \
		echo "‚ùå –û—Ç–º–µ–Ω–µ–Ω–æ"; \
	fi

# =============================================================================
# –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–¨–°–ö–ò–ï –ù–ê–°–¢–†–û–ô–ö–ò
# =============================================================================

.PHONY: create-user-schema
create-user-schema: ## –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ö–µ–º—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫
	@echo "üóÑÔ∏è –°–æ–∑–¥–∞–Ω–∏–µ —Å—Ö–µ–º—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫..."
	docker-compose exec -T postgres psql -U testuser -d testdb < user_config_schema.sql
	@echo "‚úÖ –°—Ö–µ–º–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫ —Å–æ–∑–¥–∞–Ω–∞"

.PHONY: create-test-user
create-test-user: ## –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
	@echo "üë§ –°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏..."
	python user_config_manager.py
	@echo "‚úÖ –¢–µ—Å—Ç–æ–≤—ã–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–æ–∑–¥–∞–Ω"

.PHONY: setup-user-system
setup-user-system: create-user-schema create-test-user ## –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–æ–ª–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
	@echo "üéâ –°–∏—Å—Ç–µ–º–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫ –≥–æ—Ç–æ–≤–∞!"

.PHONY: test-user-sync
test-user-sync: ## –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
	@echo "üîÑ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–π —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏..."
	python -c "
import asyncio
from user_config_manager import UserConfigManager, SyncManager

async def test_sync():
    config_manager = UserConfigManager()
    sync_manager = SyncManager(config_manager)
    
    # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤–æ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
    with config_manager.db_conn.cursor() as cursor:
        cursor.execute('SELECT id FROM users LIMIT 1')
        row = cursor.fetchone()
        if row:
            user_id = row[0]
            print(f'üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—é –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è {user_id}')
            
            task_id = await sync_manager.start_sync_task(
                user_id=user_id,
                sources=['jira', 'confluence', 'gitlab'],
                task_type='manual'
            )
            
            print(f'üìù –°–æ–∑–¥–∞–Ω–∞ –∑–∞–¥–∞—á–∞ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏: {task_id}')
            
            # –ñ–¥–µ–º –Ω–µ–º–Ω–æ–≥–æ –∏ –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç–∞—Ç—É—Å
            await asyncio.sleep(2)
            status = sync_manager.get_sync_status(task_id)
            if status:
                print(f'üìä –°—Ç–∞—Ç—É—Å: {status["status"]} ({status["progress_percentage"]}%)')
                
            logs = sync_manager.get_sync_logs(task_id)[:5]  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 –ª–æ–≥–æ–≤
            print(f'üìã –õ–æ–≥–æ–≤: {len(logs)}')
            for log in logs:
                print(f'  {log["log_level"]}: {log["message"]}')
        else:
            print('‚ùå –ù–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –≤ —Å–∏—Å—Ç–µ–º–µ')

asyncio.run(test_sync())
"
	@echo "‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–æ"

.PHONY: show-user-stats
show-user-stats: ## –ü–æ–∫–∞–∑–∞—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
	@echo "üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π..."
	python -c "
import psycopg2
import os

conn = psycopg2.connect(
    host=os.getenv('POSTGRES_HOST', 'localhost'),
    port=os.getenv('POSTGRES_PORT', 5432),
    database=os.getenv('POSTGRES_DB', 'testdb'),
    user=os.getenv('POSTGRES_USER', 'testuser'),
    password=os.getenv('POSTGRES_PASSWORD', 'testpass')
)

with conn.cursor() as cursor:
    # –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π
    cursor.execute('SELECT COUNT(*) FROM users')
    users_count = cursor.fetchone()[0]
    print(f'üë• –í—Å–µ–≥–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π: {users_count}')
    
    # –ê–∫—Ç–∏–≤–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    cursor.execute('''
        SELECT source_type, 
               COUNT(*) as total,
               COUNT(CASE WHEN is_enabled_semantic_search THEN 1 END) as enabled_search,
               COUNT(CASE WHEN is_enabled_architecture_generation THEN 1 END) as enabled_arch
        FROM user_data_sources 
        GROUP BY source_type
    ''')
    
    print('üîó –ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö:')
    for source_type, total, enabled_search, enabled_arch in cursor.fetchall():
        print(f'  {source_type}: {total} –≤—Å–µ–≥–æ | –ø–æ–∏—Å–∫: {enabled_search} | –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: {enabled_arch}')
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π
    cursor.execute('SELECT COUNT(*) FROM user_jira_configs')
    jira_configs = cursor.fetchone()[0]
    
    cursor.execute('SELECT COUNT(*) FROM user_confluence_configs')
    confluence_configs = cursor.fetchone()[0]
    
    cursor.execute('SELECT COUNT(*) FROM user_gitlab_configs')
    gitlab_configs = cursor.fetchone()[0]
    
    print('‚öôÔ∏è –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–π:')
    print(f'  Jira: {jira_configs}')
    print(f'  Confluence: {confluence_configs}')
    print(f'  GitLab: {gitlab_configs}')
    
    # –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
    cursor.execute('SELECT COUNT(*), SUM(file_size) FROM user_files')
    files_count, total_size = cursor.fetchone()
    total_size_mb = (total_size or 0) / (1024 * 1024)
    print(f'üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {files_count} ({total_size_mb:.1f} MB)')
    
    # –ó–∞–¥–∞—á–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏
    cursor.execute('''
        SELECT status, COUNT(*) 
        FROM sync_tasks 
        GROUP BY status
    ''')
    
    print('üîÑ –ó–∞–¥–∞—á–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏:')
    for status, count in cursor.fetchall():
        print(f'  {status}: {count}')

conn.close()
"
	@echo "‚úÖ –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –≥–æ—Ç–æ–≤–∞"

# =============================================================================
# –ü–û–õ–ù–ê–Ø –ò–ù–ò–¶–ò–ê–õ–ò–ó–ê–¶–ò–Ø –°–ò–°–¢–ï–ú–´
# =============================================================================

.PHONY: init-full-system
init-full-system: up setup-user-system train-model ## –ü–æ–ª–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã
	@echo "üöÄ –ü–æ–ª–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!"
	@echo ""
	@echo "üìã –î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã:"
	@echo "  make e2e-full-pipeline    - –ü–æ–ª–Ω—ã–π E2E –ø–∞–π–ø–ª–∞–π–Ω"
	@echo "  make test-feedback        - –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏"  
	@echo "  make check-model-quality  - –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏"
	@echo "  make simulate-user-feedback - –°–∏–º—É–ª—è—Ü–∏—è –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏"
	@echo "  make retrain-model        - –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏"
	@echo "  make test-user-sync       - –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è"
	@echo "  make show-user-stats      - –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π"

.PHONY: docker-up-clean
docker-up-clean: ## –ó–∞–ø—É—Å–∫ Docker —Å –æ—á–∏—Å—Ç–∫–æ–π –¥–∞–Ω–Ω—ã—Ö
	@echo "üê≥ –ó–∞–ø—É—Å–∫ Docker —Å –æ—á–∏—Å—Ç–∫–æ–π..."
	docker-compose down -v
	docker-compose up -d
	@echo "‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ —Å–µ—Ä–≤–∏—Å–æ–≤ (3 –º–∏–Ω—É—Ç—ã)..."
	sleep 180
	@echo "‚úÖ Docker —Å–µ—Ä–≤–∏—Å—ã –≥–æ—Ç–æ–≤—ã"
# User Management Commands
create-user-schema:
	docker-compose exec -T postgres psql -U testuser -d testdb < user_config_schema.sql

