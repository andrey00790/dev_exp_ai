#!/usr/bin/env python3
"""
–ò–ù–¢–ï–ì–†–ê–¶–ò–û–ù–ù–´–ô –¢–ï–°–¢ –û–°–ù–û–í–ù–´–• –°–¶–ï–ù–ê–†–ò–ï–í –û–ë–£–ß–ï–ù–ò–Ø
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —á—Ç–æ –≤—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –ø–æ–¥–∫–ª—é—á–µ–Ω—ã –∫ —Å–∏—Å—Ç–µ–º–µ –æ–±—É—á–µ–Ω–∏—è

‚úÖ Search ‚Üí Evaluation ‚Üí Feedback ‚Üí Learning
‚úÖ RFC Generation ‚Üí Quality Assessment ‚Üí Feedback ‚Üí Learning  
‚úÖ AI Analytics ‚Üí Performance Metrics ‚Üí Feedback ‚Üí Learning
‚úÖ Real API Integration ‚Üí Live Data ‚Üí Feedback Collection ‚Üí Model Retraining
"""

import asyncio
import httpx
import pytest
import time
import json
from datetime import datetime
from typing import Dict, Any, List

class MainScenariosLearningTester:
    def __init__(self, api_base_url: str = "http://localhost:8000"):
        self.api_base_url = api_base_url
        self.client = httpx.AsyncClient(timeout=120.0)
        self.results = {
            "scenarios_tested": [],
            "learning_integration_results": {},
            "feedback_collection_stats": {},
            "retraining_triggers": []
        }
        
    async def test_scenario_1_search_learning_cycle(self) -> Dict[str, Any]:
        """–¢–µ—Å—Ç –ø–æ–ª–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –æ–±—É—á–µ–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞"""
        print("\nüîç SCENARIO 1: Search ‚Üí Evaluation ‚Üí Feedback ‚Üí Learning")
        
        # –®–∞–≥ 1: –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
        search_queries = [
            "OAuth 2.0 authentication implementation",
            "–º–∏–∫—Ä–æ—Å–µ—Ä–≤–∏—Å—ã –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ –ø–∞—Ç—Ç–µ—Ä–Ω—ã", 
            "Docker deployment best practices"
        ]
        
        search_results = []
        for query in search_queries:
            search_payload = {
                "query": query,
                "limit": 5,
                "source_types": ["confluence", "gitlab"],
                "include_snippets": True
            }
            
            response = await self.client.post(f"{self.api_base_url}/api/v1/search/search", json=search_payload)
            if response.status_code == 200:
                search_data = response.json()
                search_results.append({
                    "query": query,
                    "results_count": len(search_data.get("results", [])),
                    "search_data": search_data
                })
                print(f"   ‚úÖ Search: '{query}' ‚Üí {len(search_data.get('results', []))} results")
            else:
                print(f"   ‚ùå Search failed: {query}")
        
        # –®–∞–≥ 2: –û—Ç–ø—Ä–∞–≤–ª—è–µ–º feedback –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø–æ–∏—Å–∫–∞
        feedback_sent = 0
        for search_result in search_results:
            quality_score = min(100, 30 + len(search_result["search_data"].get("results", [])) * 10)
            rating = 5 if quality_score >= 80 else 4 if quality_score >= 60 else 3
            
            feedback_payload = {
                "target_id": f"search_{hash(search_result['query']) % 10000}",
                "context": "search_result",
                "feedback_type": "like" if quality_score >= 50 else "dislike",
                "rating": rating,
                "comment": f"Integration test: Quality={quality_score}%, Query='{search_result['query']}'",
                "user_id": "integration_test"
            }
            
            response = await self.client.post(f"{self.api_base_url}/api/v1/feedback", json=feedback_payload)
            if response.status_code == 200:
                feedback_sent += 1
                print(f"   üìù Feedback sent: Quality={quality_score}%, Rating={rating}")
            else:
                print(f"   ‚ùå Feedback failed: {response.status_code}")
        
        return {
            "scenario": "search_learning_cycle",
            "searches_performed": len(search_results),
            "feedback_sent": feedback_sent,
            "success_rate": feedback_sent / max(1, len(search_results)),
            "integration_status": "connected" if feedback_sent > 0 else "disconnected"
        }
    
    async def test_scenario_2_rfc_generation_learning(self) -> Dict[str, Any]:
        """–¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ RFC –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –æ–±—É—á–µ–Ω–∏–µ–º"""
        print("\nüìù SCENARIO 2: RFC Generation ‚Üí Quality Assessment ‚Üí Learning")
        
        # –°–∏–º—É–ª–∏—Ä—É–µ–º RFC –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –∏ feedback
        rfc_scenarios = [
            {"topic": "API Gateway Design", "complexity": "high"},
            {"topic": "Microservices Communication", "complexity": "medium"},
            {"topic": "Database Optimization", "complexity": "low"}
        ]
        
        feedback_results = []
        for scenario in rfc_scenarios:
            # –°–∏–º—É–ª–∏—Ä—É–µ–º –æ—Ü–µ–Ω–∫—É –∫–∞—á–µ—Å—Ç–≤–∞ RFC
            quality_score = 75 if scenario["complexity"] == "high" else 60 if scenario["complexity"] == "medium" else 45
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º feedback –≤ —Å–∏—Å—Ç–µ–º—É –æ–±—É—á–µ–Ω–∏—è
            feedback_payload = {
                "target_id": f"rfc_{hash(scenario['topic']) % 10000}",
                "context": "rfc_generation",
                "feedback_type": "like" if quality_score >= 60 else "dislike",
                "rating": 5 if quality_score >= 80 else 4 if quality_score >= 60 else 3,
                "comment": f"RFC quality assessment: {quality_score}%, Topic: {scenario['topic']}, Complexity: {scenario['complexity']}",
                "user_id": "integration_test"
            }
            
            response = await self.client.post(f"{self.api_base_url}/api/v1/feedback", json=feedback_payload)
            if response.status_code == 200:
                feedback_results.append({"topic": scenario["topic"], "quality": quality_score, "feedback_sent": True})
                print(f"   ‚úÖ RFC Feedback: '{scenario['topic']}' ‚Üí Quality {quality_score}%")
            else:
                feedback_results.append({"topic": scenario["topic"], "quality": quality_score, "feedback_sent": False})
                print(f"   ‚ùå RFC Feedback failed: '{scenario['topic']}'")
        
        successful_feedback = sum(1 for r in feedback_results if r["feedback_sent"])
        
        return {
            "scenario": "rfc_generation_learning", 
            "rfc_scenarios_tested": len(rfc_scenarios),
            "feedback_sent": successful_feedback,
            "average_quality": sum(r["quality"] for r in feedback_results) / len(feedback_results),
            "integration_status": "connected" if successful_feedback > 0 else "disconnected"
        }
    
    async def test_scenario_3_analytics_performance_learning(self) -> Dict[str, Any]:
        """–¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∞–Ω–∞–ª–∏—Ç–∏–∫–∏ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ —Å –æ–±—É—á–µ–Ω–∏–µ–º"""
        print("\nüìä SCENARIO 3: AI Analytics ‚Üí Performance Metrics ‚Üí Learning")
        
        # –°–∏–º—É–ª–∏—Ä—É–µ–º —Å–±–æ—Ä –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
        performance_metrics = [
            {"metric": "search_response_time", "value": 150, "threshold": 200},
            {"metric": "rfc_generation_quality", "value": 65, "threshold": 70},
            {"metric": "user_satisfaction", "value": 4.2, "threshold": 4.0},
            {"metric": "feedback_volume", "value": 25, "threshold": 20}
        ]
        
        feedback_sent = 0
        for metric in performance_metrics:
            # –û—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            performance_ok = metric["value"] >= metric["threshold"] if metric["metric"] != "search_response_time" else metric["value"] <= metric["threshold"]
            quality_score = 80 if performance_ok else 40
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º feedback –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            feedback_payload = {
                "target_id": f"perf_{metric['metric']}",
                "context": "ai_question",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª—è AI –∞–Ω–∞–ª–∏—Ç–∏–∫–∏
                "feedback_type": "like" if performance_ok else "dislike",
                "rating": 5 if performance_ok else 2,
                "comment": f"Performance metric: {metric['metric']}={metric['value']}, threshold={metric['threshold']}, status={'OK' if performance_ok else 'POOR'}",
                "user_id": "performance_monitor"
            }
            
            response = await self.client.post(f"{self.api_base_url}/api/v1/feedback", json=feedback_payload)
            if response.status_code == 200:
                feedback_sent += 1
                print(f"   ‚úÖ Performance Feedback: {metric['metric']} ‚Üí {'OK' if performance_ok else 'POOR'}")
            else:
                print(f"   ‚ùå Performance Feedback failed: {metric['metric']}")
        
        return {
            "scenario": "analytics_performance_learning",
            "metrics_monitored": len(performance_metrics),
            "feedback_sent": feedback_sent,
            "performance_issues_detected": sum(1 for m in performance_metrics if (m["value"] < m["threshold"] if m["metric"] != "search_response_time" else m["value"] > m["threshold"])),
            "integration_status": "connected" if feedback_sent > 0 else "disconnected"
        }
    
    async def test_scenario_4_retraining_trigger_integration(self) -> Dict[str, Any]:
        """–¢–µ—Å—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ —Å —Å–∏—Å—Ç–µ–º–æ–π –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è"""
        print("\nüß† SCENARIO 4: Feedback Collection ‚Üí Retraining Trigger ‚Üí Model Update")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Ä–µtraining endpoints
        retraining_tests = [
            {
                "endpoint": "/api/v1/feedback/retrain",
                "payload": {
                    "context": "search_result",
                    "trigger_reason": "integration_test",
                    "min_examples": 3
                }
            },
            {
                "endpoint": "/api/v1/learning/feedback", 
                "payload": {
                    "rfc_id": "integration_test_rfc",
                    "feedback_type": "like",
                    "rating": 4,
                    "comment": "Integration test feedback for learning system"
                }
            }
        ]
        
        retraining_results = []
        for test in retraining_tests:
            try:
                response = await self.client.post(f"{self.api_base_url}{test['endpoint']}", json=test["payload"])
                
                if response.status_code == 200:
                    result_data = response.json()
                    retraining_results.append({
                        "endpoint": test["endpoint"],
                        "status": "success",
                        "response": result_data
                    })
                    print(f"   ‚úÖ Retraining endpoint: {test['endpoint']} ‚Üí SUCCESS")
                else:
                    retraining_results.append({
                        "endpoint": test["endpoint"], 
                        "status": "failed",
                        "error_code": response.status_code
                    })
                    print(f"   ‚ö†Ô∏è Retraining endpoint: {test['endpoint']} ‚Üí {response.status_code}")
                    
            except Exception as e:
                retraining_results.append({
                    "endpoint": test["endpoint"],
                    "status": "error", 
                    "error": str(e)
                })
                print(f"   ‚ùå Retraining endpoint: {test['endpoint']} ‚Üí ERROR: {e}")
        
        successful_retraining = sum(1 for r in retraining_results if r["status"] == "success")
        
        return {
            "scenario": "retraining_trigger_integration",
            "endpoints_tested": len(retraining_tests),
            "successful_triggers": successful_retraining,
            "retraining_results": retraining_results,
            "integration_status": "connected" if successful_retraining > 0 else "partial"
        }
    
    async def run_full_integration_test(self) -> Dict[str, Any]:
        """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞ –≤—Å–µ—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤"""
        print("üöÄ –ü–û–õ–ù–´–ô –ò–ù–¢–ï–ì–†–ê–¶–ò–û–ù–ù–´–ô –¢–ï–°–¢ –û–°–ù–û–í–ù–´–• –°–¶–ï–ù–ê–†–ò–ï–í –û–ë–£–ß–ï–ù–ò–Ø")
        print("=" * 80)
        
        start_time = time.time()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ API
        try:
            health_response = await self.client.get(f"{self.api_base_url}/health")
            if health_response.status_code != 200:
                print("‚ùå API not available")
                return {"status": "failed", "error": "API unavailable"}
            print("‚úÖ API is available and healthy")
        except Exception as e:
            print(f"‚ùå API connection failed: {e}")
            return {"status": "failed", "error": str(e)}
        
        # –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Å—Ü–µ–Ω–∞—Ä–∏–µ–≤
        scenarios = [
            self.test_scenario_1_search_learning_cycle(),
            self.test_scenario_2_rfc_generation_learning(), 
            self.test_scenario_3_analytics_performance_learning(),
            self.test_scenario_4_retraining_trigger_integration()
        ]
        
        scenario_results = []
        for scenario_test in scenarios:
            try:
                result = await scenario_test
                scenario_results.append(result)
                self.results["scenarios_tested"].append(result["scenario"])
                self.results["learning_integration_results"][result["scenario"]] = result
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É —Å—Ü–µ–Ω–∞—Ä–∏—è–º–∏
                await asyncio.sleep(1)
                
            except Exception as e:
                print(f"‚ùå Scenario failed: {e}")
                scenario_results.append({"scenario": "unknown", "status": "failed", "error": str(e)})
        
        total_time = time.time() - start_time
        
        # –ê–Ω–∞–ª–∏–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        connected_scenarios = sum(1 for r in scenario_results if r.get("integration_status") == "connected")
        total_feedback_sent = sum(r.get("feedback_sent", 0) for r in scenario_results)
        
        final_result = {
            "test_summary": {
                "total_scenarios": len(scenario_results),
                "connected_scenarios": connected_scenarios,
                "integration_coverage": connected_scenarios / len(scenario_results),
                "total_feedback_sent": total_feedback_sent,
                "total_time_seconds": total_time
            },
            "scenario_results": scenario_results,
            "overall_status": "FULLY_INTEGRATED" if connected_scenarios == len(scenario_results) 
                             else "PARTIALLY_INTEGRATED" if connected_scenarios > 0 
                             else "NOT_INTEGRATED",
            "recommendation": self._generate_integration_recommendation(connected_scenarios, len(scenario_results), total_feedback_sent),
            "timestamp": datetime.now().isoformat()
        }
        
        return final_result
    
    def _generate_integration_recommendation(self, connected: int, total: int, feedback_count: int) -> str:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏"""
        coverage = connected / total
        
        if coverage >= 1.0 and feedback_count >= 10:
            return "üèÜ EXCELLENT: All scenarios fully integrated with learning system"
        elif coverage >= 0.75:
            return "‚úÖ GOOD: Most scenarios integrated, minor improvements needed"  
        elif coverage >= 0.5:
            return "‚ö†Ô∏è FAIR: Partial integration, significant improvements required"
        else:
            return "üö® POOR: Critical integration issues, major fixes needed"
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –∫–ª–∏–µ–Ω—Ç–∞"""
        await self.client.aclose()

async def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    tester = MainScenariosLearningTester()
    
    try:
        # –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ —Ç–µ—Å—Ç–∞  
        results = await tester.run_full_integration_test()
        
        # –í—ã–≤–æ–¥ –∏—Ç–æ–≥–æ–≤
        summary = results["test_summary"]
        print(f"\nüìä –ò–¢–û–ì–û–í–´–ï –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ò–ù–¢–ï–ì–†–ê–¶–ò–ò:")
        print(f"   üéØ Scenarios Tested: {summary['total_scenarios']}")
        print(f"   ‚úÖ Connected Scenarios: {summary['connected_scenarios']}")
        print(f"   üìä Integration Coverage: {summary['integration_coverage']:.1%}")
        print(f"   üìù Total Feedback Sent: {summary['total_feedback_sent']}")
        print(f"   ‚è±Ô∏è Total Time: {summary['total_time_seconds']:.1f}s")
        print(f"   üèÜ Overall Status: {results['overall_status']}")
        print(f"   üìà {results['recommendation']}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"main_scenarios_learning_integration_{timestamp}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        print(f"üìÑ Full integration report saved to: {results_file}")
        
        return summary['integration_coverage'] >= 0.75
        
    except Exception as e:
        print(f"‚ùå Integration test failed: {e}")
        return False
    finally:
        await tester.close()

if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1) 